{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Tabla de contenidos<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Curso-de-introducción-al-Pensamiento-Probabilístico.\" data-toc-modified-id=\"Curso-de-introducción-al-Pensamiento-Probabilístico.-0\">Curso de introducción al Pensamiento Probabilístico.</a></span><ul class=\"toc-item\"><li><span><a href=\"#Módulo-1:-Programación-probabilística\" data-toc-modified-id=\"Módulo-1:-Programación-probabilística-0.1\">Módulo 1: Programación probabilística</a></span><ul class=\"toc-item\"><li><span><a href=\"#Introducción-a-la-programación-probabilística\" data-toc-modified-id=\"Introducción-a-la-programación-probabilística-0.1.1\">Introducción a la programación probabilística</a></span></li><li><span><a href=\"#Probabilidad-Condicional\" data-toc-modified-id=\"Probabilidad-Condicional-0.1.2\">Probabilidad Condicional</a></span></li><li><span><a href=\"#Teorema-de-Bayes\" data-toc-modified-id=\"Teorema-de-Bayes-0.1.3\">Teorema de Bayes</a></span></li><li><span><a href=\"#Entendiendo-el-Teorema-de-Bayes\" data-toc-modified-id=\"Entendiendo-el-Teorema-de-Bayes-0.1.4\">Entendiendo el Teorema de Bayes</a></span><ul class=\"toc-item\"><li><span><a href=\"#Ejemplos:\" data-toc-modified-id=\"Ejemplos:-0.1.4.1\">Ejemplos:</a></span></li><li><span><a href=\"#Enlaces-de-videos-o-artículos\" data-toc-modified-id=\"Enlaces-de-videos-o-artículos-0.1.4.2\">Enlaces de videos o artículos</a></span></li></ul></li><li><span><a href=\"#Análisis-de-síntomas\" data-toc-modified-id=\"Análisis-de-síntomas-0.1.5\">Análisis de síntomas</a></span></li><li><span><a href=\"#Aplicaciones-del-Teorema-de-Bayes\" data-toc-modified-id=\"Aplicaciones-del-Teorema-de-Bayes-0.1.6\">Aplicaciones del Teorema de Bayes</a></span><ul class=\"toc-item\"><li><span><a href=\"#Turing-y-el-código-enigma-de-los-Nazis\" data-toc-modified-id=\"Turing-y-el-código-enigma-de-los-Nazis-0.1.6.1\">Turing y el código enigma de los Nazis</a></span></li><li><span><a href=\"#Finanzas\" data-toc-modified-id=\"Finanzas-0.1.6.2\">Finanzas</a></span></li><li><span><a href=\"#Derecho\" data-toc-modified-id=\"Derecho-0.1.6.3\">Derecho</a></span></li><li><span><a href=\"#Inteligencia-artificial\" data-toc-modified-id=\"Inteligencia-artificial-0.1.6.4\">Inteligencia artificial</a></span></li></ul></li></ul></li><li><span><a href=\"#Módulo-2:-Mentiras-estadísticas\" data-toc-modified-id=\"Módulo-2:-Mentiras-estadísticas-0.2\">Módulo 2: Mentiras estadísticas</a></span><ul class=\"toc-item\"><li><span><a href=\"#Garage-in,-garbage-out-(GIGO)\" data-toc-modified-id=\"Garage-in,-garbage-out-(GIGO)-0.2.1\">Garage in, garbage out (GIGO)</a></span></li><li><span><a href=\"#Imágenes-engañosas\" data-toc-modified-id=\"Imágenes-engañosas-0.2.2\">Imágenes engañosas</a></span></li><li><span><a href=\"#Cum-Hoc-Ergo-Propter-Hoc\" data-toc-modified-id=\"Cum-Hoc-Ergo-Propter-Hoc-0.2.3\">Cum Hoc Ergo Propter Hoc</a></span></li><li><span><a href=\"#Prejuicio-en-el-muestreo\" data-toc-modified-id=\"Prejuicio-en-el-muestreo-0.2.4\">Prejuicio en el muestreo</a></span></li><li><span><a href=\"#Falacia-del-francotirador-de-Texas\" data-toc-modified-id=\"Falacia-del-francotirador-de-Texas-0.2.5\">Falacia del francotirador de Texas</a></span></li><li><span><a href=\"#Porcentajes-confusos\" data-toc-modified-id=\"Porcentajes-confusos-0.2.6\">Porcentajes confusos</a></span></li><li><span><a href=\"#Falacia-de-regresión\" data-toc-modified-id=\"Falacia-de-regresión-0.2.7\">Falacia de regresión</a></span></li></ul></li><li><span><a href=\"#Módulo-3:-Introducción-al-Machine-Learning\" data-toc-modified-id=\"Módulo-3:-Introducción-al-Machine-Learning-0.3\">Módulo 3: Introducción al Machine Learning</a></span><ul class=\"toc-item\"><li><span><a href=\"#Feature-Vectors\" data-toc-modified-id=\"Feature-Vectors-0.3.1\">Feature Vectors</a></span></li><li><span><a href=\"#Métricas-de-distancia\" data-toc-modified-id=\"Métricas-de-distancia-0.3.2\">Métricas de distancia</a></span></li><li><span><a href=\"#Profundizando-en-las-métricas-de-distancia\" data-toc-modified-id=\"Profundizando-en-las-métricas-de-distancia-0.3.3\">Profundizando en las métricas de distancia</a></span><ul class=\"toc-item\"><li><span><a href=\"#Distancia-Euclidiana\" data-toc-modified-id=\"Distancia-Euclidiana-0.3.3.1\">Distancia Euclidiana</a></span></li><li><span><a href=\"#Distancia-Chebyshev\" data-toc-modified-id=\"Distancia-Chebyshev-0.3.3.2\">Distancia Chebyshev</a></span></li><li><span><a href=\"#Distancia-de-Manhattan\" data-toc-modified-id=\"Distancia-de-Manhattan-0.3.3.3\">Distancia de Manhattan</a></span></li><li><span><a href=\"#Distancia-de-Kullback-Leibler\" data-toc-modified-id=\"Distancia-de-Kullback-Leibler-0.3.3.4\">Distancia de Kullback-Leibler</a></span></li></ul></li></ul></li><li><span><a href=\"#Módulo-4:-Agrupamiento-(clustering)\" data-toc-modified-id=\"Módulo-4:-Agrupamiento-(clustering)-0.4\">Módulo 4: Agrupamiento (clustering)</a></span><ul class=\"toc-item\"><li><span><a href=\"#Introducción-al-agrupamiento\" data-toc-modified-id=\"Introducción-al-agrupamiento-0.4.1\">Introducción al agrupamiento</a></span></li><li><span><a href=\"#Contenido-extra\" data-toc-modified-id=\"Contenido-extra-0.4.2\">Contenido extra</a></span><ul class=\"toc-item\"><li><span><a href=\"#K-Means-Clustering\" data-toc-modified-id=\"K-Means-Clustering-0.4.2.1\">K-Means Clustering</a></span></li><li><span><a href=\"#Mean-Shift-Clustering\" data-toc-modified-id=\"Mean-Shift-Clustering-0.4.2.2\">Mean-Shift Clustering</a></span></li><li><span><a href=\"#Agrupación-espacial-basada-en-densidad-de-aplicaciones-con-ruido-(DBSCAN)\" data-toc-modified-id=\"Agrupación-espacial-basada-en-densidad-de-aplicaciones-con-ruido-(DBSCAN)-0.4.2.3\">Agrupación espacial basada en densidad de aplicaciones con ruido (DBSCAN)</a></span></li><li><span><a href=\"#Agrupación-de-expectativas-maximización-(EM)-mediante-modelos-de-mezcla-gaussiana-(GMM)\" data-toc-modified-id=\"Agrupación-de-expectativas-maximización-(EM)-mediante-modelos-de-mezcla-gaussiana-(GMM)-0.4.2.4\">Agrupación de expectativas-maximización (EM) mediante modelos de mezcla gaussiana (GMM)</a></span></li><li><span><a href=\"#Agrupación-jerárquica-aglomerativa\" data-toc-modified-id=\"Agrupación-jerárquica-aglomerativa-0.4.2.5\">Agrupación jerárquica aglomerativa</a></span></li></ul></li></ul></li><li><span><a href=\"#Agrupamiento-jerárquico\" data-toc-modified-id=\"Agrupamiento-jerárquico-0.5\">Agrupamiento jerárquico</a></span></li><li><span><a href=\"#Agrupamiento-por-K-means\" data-toc-modified-id=\"Agrupamiento-por-K-means-0.6\">Agrupamiento por K-means</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Curso de introducción al Pensamiento Probabilístico.\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Módulo 1: Programación probabilística\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introducción a la programación probabilística\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* La programación probabilística utiliza probabilidades y modelos probabilísticos para ejecutar cómputos.\n",
    "\n",
    "* Se utiliza en gran cantidad de campos: investigación científica, inteligencia artificial, medicina, etc.\n",
    "\n",
    "* Existen lenguajes y librerías especializadas para ejecutar este tipo de cómputo como Pyro de Uber.\n",
    "\n",
    "La diferencia entre la programación estocástica y la programación probabilística es que, en la estocástica solo introducimos **aleatoriedad** en nuestros programas para obtener resultados como las simulaciones de Montecarlo, mientras que en la probabilística estamos trabajando **directamente de forma estadística** con nuestras probabilidades, distribuciones, etc.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un ejemplo de un caso real donde podemos aplicar este tipo de programación es con Uber. La aplicación tiene que monitorizar constantemente la ubicación de sus conductores respecto a sus correspondientes clientes además, tiene que estimar el tiempo que le tomará al conductor llegar de un punto A a un punto B, cosa qué cambiará mucho dependiendo del horario, ciudad, tráfico, etc. Para analizar esta situación hay que tener en cuenta muchas variables de las que no tenemos el control entonces, usamos un modelo probabilístico donde las incorporamos.\n",
    "\n",
    "Si lo pensamos Uber tiene una monitorización en tiempo real gracias a la aplicación que están usando sus conductores. De esta manera ya tiene los datos necesarios para tratar de predecir como son las condiciones al momento de iniciar nuestro viaje utilizando la información de los conductores que están en los alrededores.\n",
    "\n",
    "<img src=\"https://i.stack.imgur.com/8qqvK.gif\" width=\"400\" height=\"500\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otro caso de uso sería con los filtros de Spam, de hecho, fue una de las primeras aplicaciones que se le dio al machine learning en los 90's aún sin disponer del poder computacional que tenemos hoy en día. Entonces, ¿cómo lo hicieron? \n",
    "\n",
    "Para ello se utilizó programación probabilística, primero se creó un modelo de lo que significaba el Spam y lo fueron comparando con los reportes de Spam de los usuarios para incorporar esta evidencia y actualizar las probabilidades.\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/2988/1*_igArwmR7Pj_Mu_KUGD1SQ.png\" width=\"400\" height=\"500\" />\n",
    "\n",
    "\n",
    "\n",
    "[Artículo sobre una revisión general de los modelos de detección y filtrado de correo spam existentes en la\n",
    "actualidad. ](https://www.redalyc.org/pdf/925/92503405.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otro campo donde se puede utilizar es en medicina. Normalmente una consulta tradicional consiste en explicar nuestros síntomas y con base a ellos el doctor crea una hipótesis, pero esto esta en cierta parte sujeto a la especificidad con la que podemos describir nuestros síntomas.\n",
    "\n",
    "Para obtener un método que sea más eficaz y más profundo en casos requeridos se utiliza un test, el cual se encarga de ayudar al diagnóstico. Un test va a tener cierto margen de error dependiendo de lo que se trate de estudiar, algunos serán más o menos 'permisivos' a obtener falsos positivos o falsos negativos dependiendo la gravedad y la urgencia por detectar el problema. Mientras sea más sensible podrá ayudarnos a detectar mejor el problema, pero también será más sensible a ciertos problemas al detectar cosas que no nos interesan o que interrumpen la prueba. \n",
    "\n",
    "¿Y cómo entra la programación probabilística aquí? Bueno, los test no suelen ser del todo exactos por lo que dependiendo de la exactitud de un test podemos llegar a conclusiones distintas y nos será de mucha ayuda hacer ciertas inferencias estadísticas a partir de los resultados obtenidos y quizá también, de resultados anteriores.\n",
    "\n",
    "<img src=\"https://algorithmia.com/blog/wp-content/uploads/2018/08/word-image.png\" width=\"400\" height=\"500\" />\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probabilidad Condicional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recordemos que la probabilidad solo es una cantidad que nos indica la certidumbre con la que sabemos que un evento puede ocurrir. Cuando pensamos en experimentos cómo lanzar una moneda al aire, sacar una carta sin regresarla, girar una ruleta, etc. Entonces, estamos hablando de **probabilidad independiente**, es decir, los eventos no están relacionados entre sí. El alcance de este enfoque de la probabilidad es algo pequeño cuando queremos evaluar eventos como, ¿cuál es la probabilidad de A dado que ya ocurrió B? Donde nuestra acción previa esta teniendo un efecto en la probabilidad de un evento siguiente.\n",
    "\n",
    "Para expandir nuestro uso vamos a incorporar algo conocido como **Probabilidad Condicional**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Empecemos con un ejemplo de probabilidad independiente:\n",
    "\n",
    "Imaginemos un caso donde lanzamos una moneda 2 veces y queremos saber la probabilidad de que ocurra un evento en especial, por ejemplo que salgan 2 cruces seguidas. Para ello podemos hacer uso de un árbol de probabilidades donde las caras se representan con la letra *H* (Heads) y las cruces se representan con la *T* (Tails).\n",
    "\n",
    "\n",
    "<img src=\"https://qiskit.org/textbook/images/whatis/whatis3.png\" width=\"400\" height=\"500\" />\n",
    "\n",
    "\n",
    " Entonces, ¿cómo calculamos la probabilidad de que salgan 2 cruces seguidas?.\n",
    " \n",
    "Sabemos que el lanzamiento de una moneda es independiente del otro lanzamiento, por lo que en ambos lanzamientos tenemos $\\frac{1}{2}$ de probabilidad de obtener cara o cruz. Así que podemos estructurar nuestra pregunta de la forma, ¿ cuál es la probabilidad de obtener una cruz en el primer lanzamiento Y una cruz en el segundo lanzamiento?\n",
    "\n",
    "Podemos ver que la preposición Y indica que estamos hablando de que ambos eventos ocurran y se puede representar con la operación *AND*. Así que podemos multiplicar la probabilidad de cada uno de los eventos como se muestra en el gif de abajo.\n",
    "\n",
    "<img src=\"https://qiskit.org/textbook/images/whatis/whatis4.gif\" width=\"500\" height=\"600\" />\n",
    "\n",
    "Esto funciona solo cuando estamos hablando de **Probabilidad Incondicional**\n",
    "\n",
    "Se podría generalizar de la siguiente forma:\n",
    "\n",
    "$$P(A~y~B) = P(A~\\cap~B) = P(A)P(B)$$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, ¿qué pasa cuando tenemos que tener en consideración el evento anterior?\n",
    "\n",
    "En este caso tenemos que cambiar nuestra expresión anterior por la siguiente:\n",
    "\n",
    "**$$P(A~\\cap~B)= P(A)P(B|A)$$**\n",
    "\n",
    "Donde el $P(B|A)$ se lee como *Probabilidad de B dado que ocurrió A*. \n",
    "\n",
    "Y cómo en este caso B ocurre dependiendo de si A ocurre, entonces nuestra probabilidad de obtener el evento B se puede expresar se la siguiente manera:\n",
    "\n",
    "$$P(B) = P(A)P(B|A) + P(\\neg A)P(B|\\neg A)$$\n",
    "\n",
    "Donde $(\\neg A)$ se lee como **A negada o No ocurre A**\n",
    "\n",
    "Pongamos un ejemplo para que sea más claro. Pensemos en un test médico y asignemos **A** como el evento de que un test de cáncer salga positivo y **B** como el evento de que dicha persona en efecto tenga cáncer.\n",
    "\n",
    "¿Cuál es la probabilidad de que alguien tenga cáncer?\n",
    "\n",
    "$$P(cáncer) = P(Positivo)P(cáncer|Positivo) + P(Negativo)P(cáncer|Negativo)$$\n",
    "\n",
    "Podemos ver que para obtener la probabilidad individual de B estamos sumando la probabilidad de B dada que ocurra y no ocurra A ya que en ambos casos seguimos teniendo una probabilidad de que B ocurra. \n",
    "\n",
    "Por ejemplo, ¿cual es la probabilidad de que pase un examen dependiendo si estudié o no? Puede que la probabilidad de que pase sin estudiar sea muy pequeña pero aun así sea diferente de 0 y si estudié la probabilidad será mucho más grande. Por lo tanto, si queremos saber cual es la probabilidad de que pase el examen  tendremos que sumar la probabilidad de que lo pase habiendo estudiado más la probabilidad de que lo pase sin haber estudiado. En este caso pasar el examen es el evento **B** y estudiar es el evento **A**.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Teorema de Bayes\n",
    "\n",
    "El razonamiento detrás del teorema está en que nos permite incorporar evidencia para actualizar nuestras creencias previas sobre algo y actualizar la forma en la que aproximamos ciertos resultados.\n",
    "\n",
    "Supongamos que tenemos una hipótesis **H** y un evento **E**, y queremos saber la probabilidad de que nuestra hipótesis se cumpla dado que ocurrió **E**. Entonces, el Teorema de Bayes expresa lo siguiete:\n",
    "\n",
    "\n",
    "$$P(H|E) = \\frac{P(H)*P(E|H)}{P(E)}$$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Expliquemos el teorema con un ejemplo.*\n",
    "\n",
    "> Imaginemos el concepto de una persona que es muy tímida y retraída, que se interesa muy poco en relacionarse con las demás personas. Un alma mansa y ordenada, que le gusta tener todo en orden y estructura con una pasión por los detalles.\n",
    "\n",
    "Ahora, partiendo de esta descripción, ¿qué sería más probable, que esta persona sea un bibliotecario o que sea un granjero?\n",
    "\n",
    "\n",
    "<img src=\"https://i.imgur.com/wK1D2MW.png\" width=\"500\" height=\"600\"/>\n",
    "\n",
    "<img src=\"https://i.imgur.com/HuWF21G.png\" width=\"500\" height=\"600\"/>\n",
    "\n",
    "\n",
    "Nuestra intuición y creencias podrían decirnos que ese es un comportamiento que asociamos más a un bibliotecario ya que cumple con el típico estereotipo de una persona reservada que hace ese tipo de labores.\n",
    "\n",
    "Lo que no estamos teniendo en cuenta es que, en la realidad, la población de granjeros es mucho más grande que la población de bibliotecarios. Por ejemplo, hace unos años la proporción de granjeros y bibliotecarios en los Estados Unidos era de 20 a 1.\n",
    "\n",
    "<img src=\"https://i.imgur.com/dZbIsmY.png\" width=\"400\" height=\"500\"/>\n",
    "\n",
    "\n",
    "Ahora, vamos a construir una representación geométrica de esto manteniendo nuestra proporción de 20 a 1. (La proporción de las poblaciones totales)\n",
    "\n",
    "<img src=\"https://i.imgur.com/QLWkp7o.png\" width=\"400\" height=\"500\"/>\n",
    "\n",
    "\n",
    "Y según nuestra intuición anterior, podríamos decir que nuestro modelo de la persona coincide un 40% de las veces en la población de bibliotecarios y un 20% en la población de granjeros. (Ya que por eso creíamos que era más probable de que fuera un bibliotecario).\n",
    "\n",
    "<img src=\"https://i.imgur.com/hNRSTPJ.png\" width=\"500\" height=\"600\"/>\n",
    "\n",
    "\n",
    "\n",
    "Así que para saber la **probabilidad de que alguien sea un bibliotecario dada la descripción previa** se podría expresar cómo\n",
    "\n",
    "$$P(Bibliotecario|Descripción)= \\frac{4}{4+20} \\approx 16.7\\%$$\n",
    "\n",
    "Podemos darnos cuenta de que a pesar de que creamos que es el doble de probable de que sea un bibliotecario a que sea un granjero, la cantidad de individuos de cada población nos dice que aún así será una mayor cantidad de personas de la población de granjeros. Por ello decimos que estamos actualizando nuestras creencias previas, porqué después de esto tendremos a pensar que realmente influye nuestra intuición sesgada al hacer selección sobre una población y que tendremos que incorporar la cantidad de individuos de cada una y reajustar las proporciones.\n",
    "\n",
    "Podemos realizar una visualización geométrica de las proporciones de la siguiente forma:\n",
    "\n",
    "<img src=\"https://i.imgur.com/4xcOonl.png\"  width=\"400\" height=\"200\"/>\n",
    "\n",
    "\n",
    "Y matemáticamente se representa de esta forma que después analizaremos a profundidad\n",
    "\n",
    "$$P(A|B) = \\frac{P(B|A)P(A)}{P(B)}$$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Entendiendo el Teorema de Bayes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Retomando nuestro ejemplo anterior de la probabilidad de que ocurra nuestra Hipótesis dado un Evento, podemos escribirlo de la siguiente forma.\n",
    "\n",
    "$$\\begin{align}\n",
    "    P(H|E) &= \\frac{P(H~\\text{AND}~E)}{P(E)} = \\frac{P(H)P(E|H)}{P(E)}\\\\ \\\\\n",
    "    P(H|E) &= \\frac{P(H)P(E|H)}{P(H)P(E|H) + P(\\neg{H})P(E|\\neg{H})}\n",
    " \\end{align}$$\n",
    "\n",
    "Ahora pongamosle nombre a cada una de las partes: \n",
    "\n",
    "* P(H) = Prior: Conocimiento previo o hipótesis antes de recolectar evidencia.\n",
    "\n",
    "* P(E|H) = Likelihood: Certidumbre de que ocurra un evento condicionado por otro evento.\n",
    "\n",
    "* P(H) = Posterior: Actualización de las creencias gracias a los datos.\n",
    "\n",
    "Podemos ver entonces a la probabilidad no solo cómo un campo basado en el estudio del azar, sino también de proporciones. De esta forma podremos recordar con más facilidad el Teorema de Bayes recordando la imagen de abajo y deduciendo la ecuación a partir de esa idea.\n",
    "\n",
    "<img src=\"https://i.imgur.com/kiSbcMV.png\"  width=\"500\" height=\"600\"/>\n",
    "\n",
    "NOTA: El nombre de ese tipo de diagramas es \"eicosograma\" or \"eikosogram\".\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ejemplos:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Ejemplo 1:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Linda es una mujer de 31 años, soltera, de mente abierta, y muy brillante y se especializó en filosofía. Desde que era estudiante se interesó en estudiar los diferentes problemas de discriminación en el mundo y sobre cómo hacer justicia social, y además, participó en demostraciones anti-nucleares.\"\n",
    "\n",
    "**¿Qué es más probable?**\n",
    "\n",
    "1. **Que ella trabaje en un banco.**\n",
    "2. **Que ella trabaje en un banco y sea activista en un movimiento feminista.**\n",
    "\n",
    "Si crees que la respuesta es la número 2 significa que has caído en una trmapa Bayesiana. Y no sólo tu, en realidad, alrededor del 85% de las personas caen en este tipo de trampas lógicas. \n",
    "\n",
    "Lo que esta pasando aquí es de qué estamos usando nuestra información anterior para buscar validar una de las acciones que aparecen en las opciones.\n",
    "\n",
    "Lo que no estamos teniendo en cuenta es que podemos considerar a las mujeres que trabajan en un banco como un conjunto o una población total, y por lo tanto, considerar a las mujeres que trabajan en un banco **y además son activistas en un movimiento feminista** cómo un subconjunto del mismo. \n",
    "\n",
    "Es decir, la probabilidad de la segunda opción tendría que ser mucho más pequeña ya que se trata de una fracción del conjunto de todas las mujeres que trabajan en un banco, esta idea se ilustra en la imagen de abajo:\n",
    "\n",
    "<img src=\"https://i.imgur.com/7R0NT7s.png\" alt=\"Drawing\" width=\"500\" height=\"600\"/>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Ejemplo 2:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Un taxi estuvo involucrado en un atropello durante la noche y se escapó. En la ciudad operan dos compañías de taxis, los taxis verdes y los azules. Y disponemos de la siguiente información\n",
    "\n",
    "- a) El 85% de los taxis son Verdes y el 15% son azules.\n",
    "\n",
    "- b) Un Testigo identificó al taxi Azul como el causante del atropello. La corte realizó las pruebas de confiabilidad del testigo bajo las mismas circunstancias que existían la noche del accidente y concluyeron que **el testigo identificaba correctamente uno de los dos colores el 80% de las veces y fallaba el 20% de las veces\".\n",
    "\n",
    "**¿Cuál es la probabilidad de qué el taxi involucrado en el accidente fuera Azul en lugar de Verde?** (La distribución de los taxis en la ciudad se considera irrelevante).\n",
    "\n",
    "**SOLUCIÓN**: Expresemos nuestro problema de forma que tenga sentido el uso del teorema de Bayes y la probabilidad condicional.\n",
    "\n",
    "> ¿Cuál es la probabilidad de que el taxi sea azul dado que el testigo afirmó que es azul?\n",
    "\n",
    "**Datos**: \n",
    "* Del total de taxis de la ciudad 85% son Verdes y el 15% Azules.\n",
    "* El testigo acierta el 80% de las veces (es azul) y falla el 20% (era verde).\n",
    "\n",
    "**Eikosograma**: \n",
    "<img src=\"https://i.imgur.com/rORvnH0.png\" alt=\"Drawing\" width=\"500\" height=\"600\"/>\n",
    "\n",
    "**Aplicando el Teorema**:\n",
    "\n",
    "Diremos que **H** es nuestra hipótesis donde el auto es azul y **E** representa la certeza de que el testigo este en lo correcto. \n",
    "\n",
    "$$ P(\\text{Taxi sea azul}|\\text{El testigo dijo que es azul}) = P(H|E) $$\n",
    "\n",
    "\n",
    "\n",
    "$$ \\begin{align}\n",
    " P(H|E) =&\\frac{P(H)P(E|H)}{P(E)} = \\frac{0.15*0.8}{(0.15*0.8)+(0.85*0.2)}\\\\\n",
    "P(H|E)\\approx & {~41.4}\\%\n",
    " \\end{align}$$\n",
    " \n",
    " ---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Ejemplo 3: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una fábrica de celulares dispone de dos máquinas A y B que laboran el **60%** y el **40%** de la producción.\n",
    "El porcentaje de celulares defectuosos que produce cada máquina es del **5%** y del **10%** respectivamente.\n",
    "\n",
    ">**¿Cuál es la probabilidad de que el celular haya sido fabricado por la máquina A, sabiendo que es defectuoso?**\n",
    "\n",
    "Podemos imaginarnos un árbol de probabilidades de la siguiente forma:\n",
    "\n",
    "<img src=\"https://i.imgur.com/Jr1ylS7.png\" alt=\"Drawing\" width=\"500\" height=\"600\"/>\n",
    "\n",
    "Escribamos el problema de una forma más legible matemáticamente:\n",
    "\n",
    "$$ \\begin{align}\n",
    "    P(A|D) &= \\frac{P(A)P(D|A)}{P(D)}\\\\\n",
    "           &= \\frac{P(A)P(D|A)}{P(A)P(D|A) + P(\\neg{A})P(D|\\neg{A})}\\\\\n",
    "           &= \\frac{(.60)(.05)}{(.60*.05)+(.40*.10)} \\\\\n",
    "           &= \\frac{3}{7} \\approx {0.428571}   \n",
    "    \\end{align}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Enlaces de videos o artículos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "[Video que explica el teorema de bayes](https://www.youtube.com/watch?v=HZGCoVF3YvM&t=393s)\n",
    "\n",
    "[Cómo escapar de la Trampa Bayesiana](https://www.youtube.com/watch?v=D7KKlC0LOyw)\n",
    "\n",
    "[Artículo: Probability, problems and paradoxes pictured by eikosograms](https://www.math.uwaterloo.ca/~rwoldfor/papers/eikosograms/examples/paper.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Análisis de síntomas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El Teorema de Bayes tiene una gran cantidad de aplicaciones y una de ellas es en diagnósticos médicos, en donde a partir de la certidumbre de las pruebas y de la probabilidad de que se presente X enfermedad, podremos obtener un resultado mucho más certero sobre el estado de un paciente.\n",
    "\n",
    "Veamos un ejemplo con la siguiente tabla:\n",
    "\n",
    "<img src=\"https://i.imgur.com/7lXaJg6.png\" alt=\"Drawing\" width=\"500\" height=\"600\"/>\n",
    "\n",
    "A partir de esta información vamos a hacer un programa para determinar la probabilidad de que una persona tenga la enfermedad dado que tiene un síntoma.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crearemos el programa correspondiente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_bayes(prior_A, prob_B_dado_A, prob_B): #Definimos el terorema de Bayes en una función\n",
    "    return ( (prior_A * prob_B_dado_A) / prob_B )\n",
    "\n",
    "\n",
    "if __name__ == '__main__': #Definimos la entrada del programa para correrlo en la consola\n",
    "\n",
    "    # Usando la tablita del notebook\n",
    "    # https://i.imgur.com/7lXaJg6.pnghttps://i.imgur.com/7lXaJg6.png\n",
    "\n",
    "    prob_cancer = 1 / 100000\n",
    "    prob_sintoma_dado_cancer = 1 #Ya tienes cancer, prob de tener síntomas\n",
    "    prob_sintoma_dado_no_cancer = 10 / 99999\n",
    "    prob_no_cancer = 1 - prob_cancer\n",
    "\n",
    "    prob_sintoma = (prob_sintoma_dado_cancer * prob_cancer) + (prob_sintoma_dado_no_cancer * prob_no_cancer)\n",
    "\n",
    "    #Ejecutamos\n",
    "    prob_cancer_dado_sintoma = calc_bayes(prob_cancer, prob_sintoma_dado_cancer, prob_sintoma)\n",
    "    print(prob_cancer_dado_sintoma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aplicaciones del Teorema de Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El teorema de Bayes es uno de los mecanismos matemáticos más importantes de la actualidad. A grandes trasgos, nos permite medir nuestra certidumbre con respecto a un suceso tomando en cuenta el conocimiento previo y la evidencia que tenemos a nuestra disposición. El Teorema de Bayes es el motor conceptual que alimenta mucho de nuestro mundo moderno."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Turing y el código enigma de los Nazis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alan Turing es uno de los padres del cómputo moderno; pocos saben que fue gracias a él que los aliados pudieron tener una ventaja decisiva cuando Turing logró descifrar el código enigma que encriptaba todas la comunicaciones nazis; pero aún menos saben que para romper este código se utilizó el Teorema de Bayes.\n",
    "\n",
    "Los alemanes comenzaron a utilizar la máquina de la imagen para encriptar sus comunicaciones. Esta máquina, parecida a una máquina de escribir convencional y de nombre Enigma, estaba compuesta por varios rotores que iban girando a medida que se introducía el mensaje de entrada. **Estos giros hacían que a cada letra de entrada del mensaje original le correspondiera una letra de salida de manera que, al ir rotando a cada pulsación, la misma letra de entrada no tenía siempre la misma letra de salida**. Es decir, posibilitaban un código variable. Adicionalmente, los rotores se colocaban en la máquina manualmente en una posición concreta y ese, junto con algunos datos más, pero en ningún caso con alguna clave. Esta era la única configuración que se necesitaba compartir entre los poseedores de las máquinas. Dicha configuración inicial era cambiada con frecuencia. Este funcionamiento hacía imposible romper la encriptación usando métodos de fuerza bruta ya que existían millones de posibilidades y relativamente poco tiempo para el análisis de cada ciclo.\n",
    "\n",
    "Lo que hizo Turing fue aplicar el Teorema para descifrar un segmento de un mensaje, calcular las probabilidades iniciales y actualizar las probabilidades de que el mensaje era correcto cuando nueva evidencia (pistas) era presentada.\n",
    "\n",
    "En este caso, pudo aprovechar una de las vulnerabilidades de la enigma, ninguna tecla se codiciaba a si misma y esa era una excelente pista ya que solo teníamos que descartar las posiciones en las que una misma letra coincidía hasta que llegáramos a un mensaje legible. Además, gracias a la información que recopilaban en la guerra pudieron acceder a maquinas enigma dentro de un submarino de las tropas enemigas, acelerando mucho más el proceso de desencriptación.\n",
    "\n",
    "[La máquina de Turing - Javier García](https://www.youtube.com/watch?v=NS-NQ5mCSs8&t=2877s)\n",
    "\n",
    "[Video sobre la máquina enigma y cómo fue el proceso de desencriptación](https://www.youtube.com/watch?v=V4V2bpZlqx8)\n",
    "\n",
    "[Documental Alan Turing y el código enigma](https://www.youtube.com/watch?v=OfGy1PH9OZU&t=4714s)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finanzas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una de las decisiones más difíciles cuando estás manejando un portafolio de inversión es determinar si un instrumento financiero (acciones, valores, bonos, etc.) se va a apreciar en el futuro y por cuánto, o si, por el contrario se debe vender el instrumento. Los portafolios managers más exitosos utilizan el Teorema de Bayes para analizar sus portafolios.\n",
    "\n",
    "En pocas palabras, puedes determinar las probabilidades iniciales basándote en el rendimiento previo de tu portafolio o en el rendimiento de toda la bolsa y luego añadir evidencia (estados financieros, proyecciones del mercado, etc.) para tener una mayor confianza en las decisiones de compra y venta.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Derecho"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El derecho es uno de los campos más fértiles para aplicar pensamiento bayesiano. Cuando un abogado quiere defender a su cliente, puede comenzar a evaluar una probabilidad de ganar (basada en su experiencia previa, o en estadísticas sobre el número de juicios y condenados con respecto del tema legal que competa) y actualiza su probabilidad conforme vayan sucediendo los eventos del proceso jurisdiccional.\n",
    "\n",
    "Cada nueva notificación, cada prueba y evidencia que encuentre, etc. Sirve para actualizar la confianza del abogado (ejemplo de ello en el ejercicio 2 del tema: Entendiendo el Teorema de Bayes).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inteligencia artificial "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El Teorema de Bayes es central en el desarrollo de sistemas modernos de inteligencia artificial. Cuando un coche autónomo se encuentra navegando las calles, tiene que identificar todos los objetos que se encuentran en su \"campo de visión\" y determinar cuál es la probabilidad de tener una colisión. Esta probabilidad se actualiza con cada movimiento de cada objeto y con el propio movimiento del vehículo autónomo. Esta constante actualización de probabilidades es lo que permite que los vehículos autónomos tomen decisiones acertadas y eviten accidentes.\n",
    "\n",
    "En esta rama tenemos otra aplicaciones cómo por ejemplo, filtros de spam, reconocimiento de voz, motores de búsqueda, análisis de riesgo crediticio, ofertas automáticas, etc.\n",
    "\n",
    "> \"Cuando los hechos cambian, yo cambio mi opinión\" - John Maynard Keynes.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Módulo 2: Mentiras estadísticas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Garage in, garbage out (GIGO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A veces nuestros programas pueden arrojarnos errores a pesar de que nuestro programa esté bien estructurado.\n",
    "Podemos imaginar una jerarquía imaginaría de los posibles bugs que podríamos tener dentro de nuestros programas.\n",
    "\n",
    "   1. Errores de sintaxis.\n",
    "   2. Errores de lógica.\n",
    "   3. Errores del diseño o pensamiento aplicado al programa o algoritmo.\n",
    "   \n",
    "Aquí vamos a tratar de detectar los terceros. Para ello hablaremos del Garbage in, garbage out. Es decir, basura adentro, basura afuera. Si desde un inicio los datos y la forma en que los obtuvimos; de nuestro programa están mal, entonces nos arrojará un output incorrecto.\n",
    "\n",
    "Recordemos algunas cosas:\n",
    "\n",
    "   * La calidad de nuestros datos es igual de fundamental que la precisión de nuestros cómputos.\n",
    "   * Cuando los datos son errados, aunque tengamos un cómputo prístino (Que se mantiene inalterado, puro, tal como era en su forma primera u original) nuestros resultados serán erróneos.\n",
    "   * En pocas palabras: con datos errados las conclusiones serán erradas. Algo que parece evidente, pero no siempre cuándo pensamos en probabilidades.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imágenes engañosas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un error muy común es extrapolar anticipadamente conclusiones después de ver una imagen. \n",
    "\n",
    "Las visualizaciones son muy importantes para entender un conjunto de datos. Sin embargo, cuando jugamos con las escalas de estás imágenes (gráficas) podemos llegar a conclusiones incorrectas. Por esta razón **no debemos confiar en gráficas sin escalas o etiquetas**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pongamos de ejemplo la siguiente imagen\n",
    "\n",
    "<img src=\"https://i.imgur.com/LubmSMC.png\" alt=\"Drawing\" width=\"500\" height=\"600\"/>\n",
    "\n",
    "Podemos ver que la gráfica de la izquierda hace parecer que existe una diferencia mucho más grande entre ambos equipos, pero lo importante aquí es que la gráfica de la izquierda tiene una escala diferente y esta empezando en 400 para dar saltos de 50 en 50, mientras que la gráfica de la derecha comienza en 0 y da saltos de 100 en 100.\n",
    "\n",
    "Aunque en este caso parece muy evidente el error, la realidad es que se suelen utilizar muchas veces este tipo de errores al propósito para expresar información incorrecta o con intención de potenciar una diferencia que realmente no es tan grande. Por ejemplo, en gráficas de rendimiento de hardware, gráficas de algún político para manipular de cierta forma el sentido de la información, gráficas del avance o rendimiento de un proyecto, noticieros, periódicos, aplanamiento de una curva de casos de Covid,  etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Paradoja de Simpson](https://www.youtube.com/watch?v=hpbXkrm68rI): Paradoja estadística y falacia ecológica donde se pueden sacar resultados aparentemente contradictorios de los mismos datos dependiendo en cómo se agrupen. La paradoja puede aparecer en estudios clínicos, exámenes estudiantiles, y un montón de sitios más.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cum Hoc Ergo Propter Hoc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Dos variables están positivamente correlacionadas cuando se mueven en la misma dirección y negativamente correlacionadas cuando se mueven en direcciones opuestas.\n",
    "* Correlación no implica causalidad, esta correlación de mide de -1 a 1.\n",
    "* Pueden existir variables escondidas o no contempladas que generen la correlación.\n",
    "* **Después de esto, eso; entonces a consecuencia de esto, eso**.\n",
    "\n",
    "La forma de escapar de este error es explorar la mayor cantidad de causas posibles y la posible relación entre cada una de ellas.\n",
    "\n",
    "> En el campo de las finanzas específicamente en la **teoría de portafolios** al analizar el comportamiento de acciones individuales y su relación con otras acciones o índices, podemos encontrar que usualmente están fuertemente **correlacionadas** en determinados periodos de tiempo, lo que nos podría llevar a tomar decisiones equivocadas en la composición de nuestro portafolio, pero al ampliar nuestro horizonte temporal podemos encontrar que **frecuentemente no existe causalidad** entre el comportamiento de las acciones analizadas, **sino que en ese periodo de tiempo se dio la casualidad de que ambas acciones se vieron afectadas por una variable macro-económica más relevante**, por lo que para resolver el problema se hace un análisis fundamental de los factores que pudieran estar afectando al portafolio y **se ponderan estos factores** para entender la magnitud en que estas variables afectan a las acciones individuales, de esta manera podemos pronosticar el comportamiento futuro de nuestro portafolio no solo basándonos en un indicador sino en una serie de factores que si pueden ser causa del comportamiento.\n",
    "Como dato adicional en los portafolio no solo se buscan correlaciones fuertes cercanas a 1, sino que como método de cobertura también se buscan correlaciones negativas cercanas a -1 para que si una acción llega a perder valor, está pérdida se compense con la ganancia de la acción con comportamiento contrario. - Comentario de clase de Jorge Díaz Lara"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un ejemplo evidente de que la correlación no implica causalidad, pero posiblemente se deban al mismo factor. Por ejemplo, un aumento de temperatura en la zona.\n",
    "\n",
    " <img src=\"https://pbs.twimg.com/media/EDFyOM6XsAAsQUp.jpg\" alt=\"Drawing\" width=\"500\" height=\"600\" />   \n",
    " \n",
    "[Página con correlaciones absurdas](http://www.tylervigen.com/spurious-correlations)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prejuicio en el muestreo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recordemos que al hacer inferencias estadísticas válidas necesitamos de muestras aleatorias y representativas. Ya que al no tenerlas, estaremos sesgados a una fracción de la población que comparte cierto comportamiento o propiedad y nos traerá errores al realizar la inferencia.\n",
    "\n",
    "De aquí es donde sale el prejuicio al momento de hacer el muestreo\n",
    "\n",
    "* Para que un muestreo pueda servir como base para la inferencia estadística tiene que ser aleatorio y representativo.\n",
    "* El prejuicio en el muestreo elimina la representatividad de las muestras.\n",
    "* A veces conseguir muestras es difícil, por lo que se utiliza la población de más fácil acceso (caso estudios universitarios).\n",
    "\n",
    "El sesgo muestral implica pre o post selección de muestras que pueden incluir preferencia o excluir cierto tipo de resultado. Normalmente esto hace que medidas de significación estadística parezcan más fuerte de lo que son. Pero también es posible causar **artefactos totalmente ilusorios**. El sesgo muestral puede ser el resultado de fraudes informativos o científicos que manipulan directamente le información, pero más a menudo es inconsciente, o bien, debido a los sesgos en los instrumentos utilizados para la observación.\n",
    "\n",
    "Para algo muy similar al manipular o contrariar opiniones a través de redes sociales. Cuando se hace una publicación que expresa cierta opinión usualmente las personas que van a reaccionar positivamente o que van a compartir dicha publicación serán personas que están de acuerdo con las ideas expresadas en el, y al final puede darles el sesgo de que su opinión es la correcta y que todos la aceptan cuando el realidad solo se están basando en una cierta parte de la población que comparte las mismas ideas que ellos como si estuvieran basando sus inferencias en una burbuja de personas que mayoritariamente están de acuerdo con ellas. De esta forma es como se han podido incluso manipular elecciones presidenciales como en Estados Unidos en 2016.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Falacia del francotirador de Texas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Esta falacia básicamente consiste en no tomar en consideración el papel de la aleatoriedad.\n",
    "* También sucede cuando uno se logra enfocar en las similitudes e ignora las diferencias.\n",
    "* Puede pasarnos al recolectar muchos datos sin tener una hipótesis previa y tratar de buscar patrones en los datos cuando quizá no existen o son patrones que no tienen nada que ver con nuestro objetivo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La falacia del francotirador de Texas trata sobre una persona que primero dispara a una pared y una vez tiene la marca de sus disparos, pinta la diana haciendo creer que atinó al centro. \n",
    "\n",
    "<div>\n",
    "<img src=\"https://i.imgur.com/hGPLW2r.png\" width=\"400\"/>\n",
    "</div>\n",
    "Puede parecer muy bobo, pero es muy fácil caer en esta falacia. Especialmente si estamos trabajando en Data Science (**adecuamos la hipótesis a los datos y no tomamos en cuenta la aleatoriedad**).\n",
    "\n",
    "[Video sobre la falacia del francotirador de Texas](https://www.youtube.com/watch?v=_tcBsryYd6s)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Porcentajes confusos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿En que momento podemos llegar a \"mentir\" con nuestros números?\n",
    "\n",
    "* Cuando no sabemos la cuenta total de la cual se obtiene un porcentaje.\n",
    "* Cuando no conocemos el contexto.\n",
    "* Los porcentajes, en vacio, no significan mucho."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos un ejemplo:\n",
    "\n",
    "* Escuela A incrementó su rendimiento un 25%\n",
    "* Escuela B incrementó su rendimiento un 10%\n",
    "* Escuela C incrementó su rendimiento en 5%\n",
    "\n",
    "No podemos adelantarnos y concluir que la escuela A es la mejor, ya que no sabemos ni siquiera, cómo se obtuvo el rendimiento, cuántos alumnos tienen, cómo se midió, cuales fueron los números anteriores del rendimiento, etc.\n",
    "\n",
    "Podemos visualizar los datos en la siguiente tabla\n",
    "<div>\n",
    "<img src=\"https://i.imgur.com/W58NcGG.png\" width=\"500\" height=\"600\"/>  \n",
    "</div>\n",
    " \n",
    "Los datos no mentían en el sentido estricto, por que realmente hubo ese incremento en cada una de las escuelas. El problema esta en que estamos partiendo de números muy distintos para cada caso, donde en realidad, las 3 escuelas tuvieron el mismo incremento en relación con el año anterior, pero la escuela A siempre se ha mantenido en uno de los lugares más bajos de rendimiento, la escuela B se mantiene a la mitad de la escala y la escuela C destaca por mucho sobre esas 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos otro ejemplo\n",
    "\n",
    "* En 1970, 12.5 millones de jóvenes vivían con sus padres.\n",
    "* En 2015 esta cifra se incrementó a 18.6 millones.\n",
    "* ¿Esto representa un incremento del 48%?\n",
    "\n",
    "De nuevo, no contamos con la información y el contexto suficiente para llegar a una conclusión clara. No sabemos, por ejemplo, cuanto creció la población de jóvenes de 1970 a 2015.\n",
    " <div>\n",
    " <img src=\"https://i.imgur.com/l2nIsXi.png\" width=\"500\" height=\"600\"/>  \n",
    " </div>\n",
    "Podemos ver que si hubo un aumento, pero del .68%.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Falacia de regresión"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Muchos eventos fluctúan naturalmente, por ejemplo, la temperatura promedio de una ciudad, el rendimiento de un atleta, los rendimientos de un portafolio de inversión, etc.\n",
    "* Cuando algo fluctúa y se aplican medidas correctivas se puede creer que existe un vínculo de causalidad en lugar de una regresión a la media, es decir, creer que si ocurre un evento extremo tiene que ocurrir otro evento extremo para \"nivelar la media\", cuando en realidad el siguiente evento sigue siendo independiente de si ocurrió o no el anterior. Esto suele pasar mucho con las inversiones, con rendimientos, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pongamos el siguiente ejemplo:\n",
    "\n",
    "Hay una ciudad que está catalogada como la número 1 de su estado en el rank de ciudades con mayor número de accidentes de tráfico, y las autoridades pretenden buscarle una solución. Así que, se opta por instalar cámaras de tráfico disuasorias. \n",
    "\n",
    "El método propuesto es el siguiente, se realiza una estadística para identificar los puntos en los que el número de accidentes durante el trimestre anterior fue muy superior a la media. Se detectan unos 50 puntos \"alarmantes\" y colocan cámaras unos metros adelante de ellas. Esto con la hipótesis de que el peligro de ser multados hará que los conductores opten por bajar su velocidad.\n",
    "\n",
    "Pasa el primer trimestre después de la instalación y parece arrojar resultados excelentes: Se ha reducido la cantidad de incidentes en esas zonas un 36%, lo que parece indicar la efectividad del resultado.\n",
    "\n",
    "Hasta aquí puede parecer muy lógico el razonamiento anterior y parece indicar que realmente existe una causalidad entre ambos hechos.\n",
    "\n",
    "Analicemos el caso.\n",
    "\n",
    "\n",
    "<div>\n",
    "<img src=\"https://naukas.com/fx/uploads/2014/05/CAMARAS.jpg\" width=\"400\"/>\n",
    "</div>\n",
    "\n",
    "Podría ser que las cámaras de tráfico realmente sean efectivas y hayan ayudado en parte a esa reducción del 36%, pero estamos dejando de lado que muchos de esos resultados se pueden explicar teniendo en cuenta la regresión a la media que experimenta el fenómeno en cuestión por mero azar.\n",
    "La clave para detectar la falacia está en la distribución de los puntos de \"mayor\" riesgo donde detectábamos los accidentes. \n",
    "\n",
    "Vamos a imaginar que tenemos una red de carreteras dividida en tramos. Y supongamos que todos los tramos son igual de peligrosos o que tienen la misma probabilidad de sufrir accidentes en dichas secciones. Calculando la media de accidentes con los datos de un trimestre y por poner un número, llegamos a que tenemos una media de 30 accidentes por tramo, en donde 30 sería nuestro valor esperado y de vez en cuando nos encontraremos con tramos con extremos de mayor o menor número de accidentes, pero recordemos que los tramos en un inicio son igual de peligrosos, por lo que esas fluctuaciones o extremos serían mero producto del azar.\n",
    "\n",
    "Recordemos que la media es de 30 accidentes, por lo que si un tramo A presentó anteriormente un registro de 3 accidentes y un tramo B presentó un registro de 90 casos. En un inicio dado los datos de A dan la apariencia de que es una carretera \"segura\", mientras que B puede parecer \"una de las más peligrosas\". \n",
    "\n",
    "Si justo de pues de la instalación de las cámaras volvemos a hacer la medición, por puro azar, veremos que la carretera A tendera a regresar a la media, subiendo de 3 a 30 es decir, subiendo unas 10 veces. Y en el caso de B veremos una disminución de 90 a 30, es decir, reduciendo unas 3 veces el \"Peligro\".\n",
    "\n",
    "¿Esto representa causalidad?,¿Podemos decir que las cámaras fueron la causa de esa disminución en B y aumento en A? No, solo estamos presenciando un comportamiento meramente del azar donde estamos presenciando una regresión a la media.\n",
    "\n",
    "[Fuente: El \"A mí me funciona\" y la falacia de la regresión a la media.](https://naukas.com/2014/06/03/el-mi-funciona-y-la-falacia-de-la-regresion-la-media/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algo muy similar ocurría en el rendimiento de jugadores de fútbo. Había una revista que se dedicaba a publicar los resultados de los jugadores con el mejor rendimiento jamás visto y la gente creía que había una maldición, por que todos los jugadores que publicaban en dicha revista comenzaban a presentar una disminución en su rendimiento muy lejana al rendimiento que los posicionó en la revista.\n",
    "\n",
    "¿Esto era una maldición?, ¿O simplemente una regresión a la media natural de cualquier atleta después de una racha increíble? Pensemoslo, si una persona de pronto tiene una racha que sobrepasa cualquier expectativa en una racha deportiva, laboral, etc. No puedes esperar que se mantenga así por siempre, es natural que tienda a regresar a su comportamiento natural de rendimiento y eso no lo hace peor atleta, solo es un ser humano que tiene un comportamiento que **varía por su propia naturaleza**.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Módulo 3: Introducción al Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con Machine Learning nos referimos al campo de estudio que le da a las computadoras la habilidad de aprender sin ser explícitamente programadas para dicha tarea.\n",
    "\n",
    "Anteriormente ya exploramos una de las ideas más importantes en el desarrollo del Machine Learning y es, el teorema de Bayes. Recordemos que este teorema nos ayuda a reincorporar la evidencia o datos para actualizar nuestras creencias. Es decir, es una operación que podemos hacer de forma recursiva (recibe los reusltados anteriores como entradas).\n",
    "\n",
    "Además, Alan Turing nos dio una primera aproximación de como pueden aprender las computadoras. Y nos demostró que cualquier algoritmo, por más complejo que sea, puede ser representado por un cómputo realizado en una máquina de turing.\n",
    "\n",
    "Hasta aquí ya veíamos el nacimiento de lo que hoy conocemos como la teoría de la computación moderna."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Después tuvimos a personajes como Marvin Minsky, quien es considerado uno de los padres de la inteligencia artificial ya que fue quien creo la primera red neuronal en los años 1950. Recordemos que para la fecha las computadoras aún eran muy primitivas comparadas con los computadores actuales, especialmente al nivel de la memoria que poseían. Fue esta la razón por la cual, se tuvo que limitar esta red neuronal a trabajar con una sola capa de información, pero ya estábamos conociendo los primeros indicios de que una computadora podría aprender ciertos patrones \"por si sola\".\n",
    "\n",
    "Minsky contribuyó posteriormente al desarrollo de la descripción gráfica simbólica, geometría computacional, representación del conocimiento, semántica computacional, percepción mecánica y al aprendizaje simbólico y conexionista."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Arthur Samuel también contribuyó mucho en el área al crear el primer programa que sabía jugar damas chinas, lo que hacía especial a este programa era que trataba de predecir las siguientes jugadas, pero de cierta forma, replicando comportamientos o jugadas desde una perspectiva humana, es decir, **memorizando y generalizando**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Frank Rosenblatt hizo otra notable contribución en las redes neuronales con la creación del perceptrón, la cual era una máquina que podía reconocer números escritos a mano para después pasar a ser representados a bits dentro de un computador.\n",
    "\n",
    "Posteriormente se generalizo este concepto y ahora, el perceptrón, dentro del campo de las redes neuronales; haca referencia a:\n",
    "\n",
    "* La neurona artificial o unidad básica de inferencia en forma de discriminador lineal, a partir de lo cual se desarrolla un algoritmo capaz de generar un criterio para seleccionar un sub-grupo a partir de un grupo de componentes más grande. \n",
    "\n",
    "\n",
    "<div>\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/b/b0/Perceptr%C3%B3n_5_unidades.svg/600px-Perceptr%C3%B3n_5_unidades.svg.png\" width=\"400\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En 1963 conocimos la primera red adversaria, las cuales consisten en algoritmos de aprendizaje no supervisado donde, por así decirlo, dejamos a las máquinas jugando por ejempli, un juego de mesa como el ajedrez; y al paso de varias partidas van aprendiendo las reglas del juego y aprenden nuevas tácticas para mejorar. En el caso de la red adversaria de 1963 fue diseñada para jugar gato o 3 en raya."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unos años después en 1967 nace el algoritmo de Nieves Neighbours el cual es un algoritmo que trata de aprender patrones a partir de un dataset de entrada. Su primera aplicación fue para calcular las rutas más óptimas entre un punto A y B."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otro punto clave fue la publicación en 1969, del libro **Perceptrons** de Marvin Minsky. Este libro fue un hito en el campo del machine learning ya que, prácticamente replanteó el paradigma en ese entonces actual, sobre el cuál se estaban construyendo los modelos de inteligencia artificial. Demostró matemáticamente, que el concepto bajo el cual se construían las redes que el mismo había desarrollado limitarían en un futuro el avance de las mismas, ya que por su propia naturaleza estaban limitadas.\n",
    "\n",
    "El problema de la perspectiva de Minsky fue, que no se percató de que se podían construir redes que estuvieran compuestas de \"capas escondidas\", por lo que, al estar estudiando desde la perspectiva de una sola capa era evidente que se encontraría con ciertas limitaciones en la construcción de estos modelos.\n",
    "\n",
    "Lastimosamente, al tratarse de uno de los entes principales dentro de la investigación de las redes neuronales: provocó que se detuviera durante casi 20 años el avance de estas, hasta que se demostró que Minsky no estaba observando el problema desde la perspectiva correcta."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En 1997 la computadora de IBM \"Deep Blue\"; que era una supercomputadora creada por IBM específicamente diseñada para jugar ajedrez; venció al campeón de ese entonces, Gary Kaspárov. Marcando un gran hito en el campo de los modelos de aprendizaje automatizado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un año después, en 1998; se hizo publico el dataset [MNIST](https://en.wikipedia.org/wiki/MNIST_database) que es un gran set de datos con muchos dígitos escritos a mano junto con las etiquetas de estos dígitos de forma que le brindaban a otros algoritmos la capacidad de exponenciar sus capacidades de cómputo a partir de este set de datos.\n",
    "\n",
    "Tiempo después, en 2009; Fei-Fei Li, una inverstigadora de Stanford que ayudó a liderar el campo de machine learning en Google; hizo publico el data set [ImageNet](https://en.wikipedia.org/wiki/ImageNet), el cual es un set de imágenes que nos permite acelerar el desarrollo de algoritmos de inteligencia artificial para el análisis de imágenes en lo que hoy en día llamamos Deep Learning.\n",
    "\n",
    "Ella se percató de que, los algoritmos no podían llegar a ser los suficientemente precisos si no tenían acceso a imágenes de alta calidad junto con etiquetas de alta calidad asociada a cada una de ellas. Así que, al liberar el dataset de ImageNet comenzamos a ver una gran explosión en el desarrollo de nuevos algoritmos de machine learning. Permitiendo que más personas pudieran acceder al campo del machine learning y, posteriormente, que el machine learning llegara a los consumidores de x o y productos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hoy en día ya estamos acostumbrados a que la inteligencia artificial a incursionado dentro de casi cualquier juego por más abstracto que parezca, pero uno de los últimos hitos que presenciamos en el pasado y que volvió a remarcar el poder computacional del que ya disponíamos gracias al machine learning, fue cuando el algoritmo **AlphaGo** derrotó al mejor jugador de **go**, un juego que es considerado cómo uno de los más complejos por la gran de posibilidades que hay que tener en consideración en cada jugada y por que se solía decir que era un juego que requería de una maestría en el dominio de compresión lógica y capacidad analítica.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Desde ese entonces el ser humano perdió el dominio en cualquier actividad o juego que involucrara sistemas cerrados, es decir, sistemas que están basados por una serie de reglas o axiomas a seguir para buscar la salida o victoria más óptima."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalizando este pequeño bosquejo histórico sobre el machine learning podemos concluir lo siguiente:\n",
    "\n",
    "* Utilizamos Machine Learning cuando:\n",
    "    * Programar un algoritmo para cierta tarea es casi imposible, o queremos buscar patrones que son muy abstractos como para codificar.\n",
    "    * El problema es muy complejo y no se conocen algoritmos para resolverlos.\n",
    "    * Ayudar a los humanos a entender patrones (data mining).\n",
    "* Aprendizaje supervisado vs no supervisado vs semisupervisado\n",
    "* Batch vs online learning (El modelo se genera una vez y se aplica, o se va actualizando constantemente con los nuevos datos). \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hay 2 conceptos que son fundamentales en el machine learning ya que a partir de uno de estos conceptos se crean la mayoría de algoritmos.\n",
    "\n",
    "Estos conceptos son:\n",
    "\n",
    "* Feature Vectors\n",
    "* Distancia \n",
    "\n",
    "Por ahora nos concentraremos en el primero, los **feature vectors**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Se utilizan para representar características simbólicas o numéricas llamadas *features*.\n",
    "* Permiten analizar un objeto desde una perspectiva matemática.\n",
    "* Los algoritmos de machine learning típicamente requieren representaciones numéricas para poder ejecutar el cómputo.\n",
    "* Uno de los *feature vectors* más conocidos es la representación del color a través del RGB.\n",
    "\n",
    "Básicamente de lo que se encargan es de ayudarnos a definir y de cierta forma cuantificar los aspectos relevantes para nuestro algoritmos. Debemos tener cuidado cuando utilicemos números absolutos muy alejados entre sí, ya que pueden desproporcionar a nuestro vector y llevarnos a conclusiones incorrectas.\n",
    "\n",
    "Podemos implementar nuestro feature vectors para analizar:\n",
    "\n",
    "* Procesamiento de imágenes\n",
    "    * Vectores: Gradientes, bordes, áreas, colores, etc.\n",
    "* Reconocimiento de voz\n",
    "    * Vectores: Distancia de sonidos, nivel de ruido, razón ruido/señal, etc.\n",
    "* Filtros anti-spam\n",
    "    * Vectores: Dirección IP, estructura del texto, frecuencia de palabras, encabezados, etc.\n",
    "    \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Métricas de distancia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las métricas de distancia nos permiten cuantificar cuan cercanos o lejanos están los vectores (variables del algoritmo) entre sí.\n",
    "\n",
    "* Muchos de los algoritmos de machine learning pueden clasificarse como algoritmos de optimización.\n",
    "* Lo que desean optimizar es una función que en muchas ocaciones se refiere a la distancia entre feature vectors.\n",
    "\n",
    "    * $x=(a,b)$ , $y=(c,d)$\n",
    "* Distancia euclidiana entre dos puntos: En linea recta\n",
    "\n",
    "    * $d=\\sqrt{(a-c)^2+(b-d)^2}$\n",
    "    \n",
    "<div>\n",
    "<img src=\"https://colalg.math.csusb.edu/camdemo/coordinateplane/images/distmov.gif\" width=\"400\"/>\n",
    "</div>\n",
    "\n",
    "* Distancia de Manhattan: Caminos posibles como al caminar en una calle\n",
    "\n",
    "    * $\\mid a-c \\mid + \\mid b-d \\mid $\n",
    "    \n",
    "<div>\n",
    "<img src = \"https://media.springernature.com/full/springer-static/image/art%3A10.1007%2Fs10462-019-09712-9/MediaObjects/10462_2019_9712_Fig3_HTML.png\" style=\"width:350px;\"/>\n",
    "</div>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Profundizando en las métricas de distancia "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matemáticamente una distancia es una función, $d(a,b)$ que asigna un número positivo a cada par de puntos en un espacio $n$-dimensional, $a=(a_1,a_2,...,a_n)$, y verifica las siguientes propiedades:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **NO negativa**, el valor nunca puede ser menor a cero: \n",
    "\n",
    "$$d(a,b) \\geq 0$$\n",
    "\n",
    "* **Simétrica**, garantizando que la distancia entre $a$ y $b$ sea la misma que entre $b$ y $a$: \n",
    "\n",
    "$$d(a,b) = d(b,a)$$\n",
    "\n",
    "* **Verifica la desigualdad triangular**, las distancias entre dos puntos ha de ser mayor o igual que la suma de las distancias de los puntos originales a un punto medio. Es decir, la distancia que hay que recorrer en dos caras de un triangulo es siempre mayor o igual la de la otra cara (distancia). Ya que si $d(a,b)$ es mayor, entonces significa que existe una distancia más corta entre $a$ y $b$:\n",
    "\n",
    "$$d(a,b) \\leq d(a,c)+d(c+b)$$\n",
    "    \n",
    "<div>\n",
    "<img src = \"https://lh3.googleusercontent.com/proxy/IbSlqbiSsvHuFBuw7D4mEUTEhjHq6w-6jgQhXGRET0DqoVT-6DRFFnETvBS2EgXw1UMY3D76_xxsK3mG008PnvPA6Nn0ONyyjjQBr27gUX_4VNrXE-Vz1PCG4V4\" style=\"width:400px;\"/>\n",
    "</div>\n",
    "\n",
    "* La distancia con el mismo punto es cero:\n",
    "\n",
    "$$d(a,a)=0$$\n",
    "\n",
    "* Para que la distancia entre dos puntos sea cero estos han de ser el mismo, es decir:\n",
    "\n",
    "$$d(a,b)=0 \\implies a =b $$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distancia Euclidiana"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es la distancia en linea recta o en la trayectoria más corta posible entre 2 puntos:\n",
    "\n",
    "$$\\mathcal{D}_{Euc}(x_i,x_j)=\\sqrt{\\sum_{r=1}^{p}{(x_{ri}-x_{rj})^2}}$$\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distancia Chebyshev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es una métrica definida en un espacio vectorial donde la distancia entre dos vectores es el mayor de sus diferencias **a lo largo de cualquier dimensión de coordenadas**:\n",
    "\n",
    "$$\\mathcal{D}_{\\mathcal{Cheb}}(i,j)= \\text{max}_k \\mid X_{ik} - X_{jk} \\mid$$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distancia de Manhattan\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La función de la distancia Manhattan calcula la distancia que se puede recorrer para llegar de un puto A a un punto B siguiendo un camino en forma de rejilla. La distancia de Manhattan entre dos elementos es la suma de las diferencias de sus correspondientes componentes:\n",
    "\n",
    "$$\\mathcal{D}_{\\mathcal{Man}}(X,Y)= \\sum_{i=1}^{k}{\\mid X_i - Y_i \\mid }$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src = \"https://iq.opengenus.org/content/images/2018/12/distance.jpg\" style=\"width:400px;\"/>\n",
    "</div>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distancia de Kullback-Leibler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es una medida no simétrica de la similitud o diferencia entre dos funciones de distribución de probabilidad P y Q. KL mide el número esperado de extra bits requeridos en muestras de P cuando se usa un código basado en Q, en lugar de un código basado en P. Generalmente P representa la \"verdadera\" distribución de los datos, observaciones, o cualquier distribución teórica. La medid Q generalmente representa una teoría, modelo, descripción o aproximación de P:\n",
    "\n",
    "$$\\mathcal{D}_{\\mathcal{Kullback}}(p,q)= \\sum_{i=1}^{n}{p(x_i)\\log\\frac{p(x_i)}{q(x_i)}}$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src = \"https://upload.wikimedia.org/wikipedia/commons/thumb/a/a8/KL-Gauss-Example.png/320px-KL-Gauss-Example.png\" style=\"width:400px;\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fuentes: \n",
    "\n",
    "[Comparación de métricas de distancia en el\n",
    "algoritmo K-Vecinos Más Cercanos para el\n",
    "problema de Reconocimiento Automático de\n",
    "Dígitos Manuscritos](http://opac.pucv.cl/pucv_txt/txt-3000/UCD3128_01.pdf)\n",
    "\n",
    "[Five most popular similarity measures implementation in Python](https://dataaspirant.com/five-most-popular-similarity-measures-implementation-in-python/)\n",
    "\n",
    "[A Comparison Study on Similarity and Dissimilarity Measures in Clustering Continuous Data](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0144059)\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Módulo 4: Agrupamiento (clustering)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introducción al agrupamiento "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Es un proceso mediante el cual se agrupan objetos similares en clusters que los identifican.\n",
    "* Se clasifica como **aprendizaje no supervisado** ya que no requiere la utilización de etiquetas.\n",
    "* Permite entender la estructura de los datos y la similitud entre los mismos.\n",
    "* Es utilizado en motores de recomendación, análisis de redes sociales, análisis de riesgo crediticio, clasificación de genes, riesgos médicos, etc.\n",
    "\n",
    "NOTA: Cluster = grupo/clase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contenido extra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K-Means Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este es probablemente el algoritmo de clustering más conocido. Se enseña en un montón de clases introductorias de data science y machine learning. Es muy fácil de entender y de implementar en código. \n",
    "\n",
    "Veamos una ilustración antes de explicarlo:\n",
    "\n",
    "<div>\n",
    "<img src = \"https://miro.medium.com/max/480/1*KrcZK0xYgTa4qFrVr0fO2w.gif\" style=\"width:400px;\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Primero, vamos a seleccionar el número de clases o grupos en los que queremos dividir nuestros datos (diferentes colores en la gráfica) e inicializamos aleatoriamente sus respectivos puntos centrales ($X$'s). Estos puntos centrales son vectores de la misma longitud que los vectores en forma  de puntitos que representan nuestros datos, en el gráfico de arriba los puntos centrales son las $X$ de colores que se van moviendo.\n",
    "\n",
    "\n",
    "2. Cada punto de datos es clasificado según un cálculo de la distancia de ese punto respecto a cada punto central ($X$'s) y lo clasifica en el grupo al que pertenece el centro más cercano.\n",
    "\n",
    "\n",
    "3. Basándonos en estos puntos ya clasificados, recalculamos el punto central tomando la media de todos los vectores en ese grupo.\n",
    "\n",
    "\n",
    "4. Repetimos los pasos anteriores para cierto número de iteraciones o hasta que los centros solo se muevan cantidades muy pequeñas. Otra manera en la que lo podemos realizar es inicializando aleatoriamente los centros varias veces y después, seleccionar el caso que nos dio mejores resultados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-Means tiene la ventaja de que es muy rápido, ya que **lo único que estamos haciendo es calcular las distancias entre los puntos y los centros de cada grupo**. Al analizar la complejidad computacional de este algoritmo vemos que tiene un Big-O de O(n), es decir, lineal.\n",
    "\n",
    "Por otro lado, K-Means tiene ciertas desventajas. Por ejemplo, tú tendrías que seleccionar previamente cuantos grupos o clases quieres, por lo que tendrías que tenerlo claro al momento de implementar el algoritmo y esa no es una tarea fácil en todos los casos. Además de que lo que nos gustaría hacer con un algoritmo de agrupamiento es que nos dé indicios de cuantos grupos podemos armar a partir de los datos.\n",
    "\n",
    "Otra desventaja, es que al depender de una inicialización aleatoria de los centros a veces podemos encontrarnos con resultados de ordenamientos que se diferencian mucho entre un punto inicial y otro. Por lo que, no tenemos asegurado que obtendremos resultados repetibles y consistentes.\n",
    "\n",
    "Hay otra alternativa a K-Meas llamada K-Medians, que es otro algoritmo de agrupación con la diferencia de que, en lugar de recalcular los puntos centrales a través de la media; en este caso utiliza el vector media del grupo. Este método es menos sensible a valores extremos o atípicos, pero es mucho más lento cuando se trata de datasets muy grandes ya que primero se requiere ordenas los datos para cada iteración al computar el vector medio.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mean-Shift Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El clustering por Mean-Shift o cambio medio, es un algoritmo basado en el principio de \"ventana deslizante\" que se encarga de encontrar áreas con concentraciones densas de datos. Es un algoritmo basado en centroides, es decir, su objetivo es localizar el centro de cada grupo o clase de datos, atrapando en un una región aquellas secciones donde hay mayor concentración de datos con características similares. De forma que va seleccionando distintos \"candidatos\" para tomarlos como los centros o media de esa área en particular de datos. Una vez procesados esos datos se pasan por una especie de filtro que se asegura de eliminar los datos duplicados para formar al conjunto final de puntos centrales junto con sus grupos correspondientes.\n",
    "\n",
    "<div>\n",
    "<img src = \"https://miro.medium.com/max/324/1*bkFlVrrm4HACGfUzeBnErw.gif\" style=\"width:400px;\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Para explicar este algoritmo consideremos un set de puntos cualesquiera en un espacio de 2 dimensiones como en la imagen de arriba. Comenzaremos con una ventana circular que se se irá recorriendo, inicialmente centrada en un punto $C$ seleccionado aleatoriamente y de radio $r$. Este algoritmo de lo que se encarga es de \"mover\" la ventana de forma iterativa de tal forma que cada vez ancerremos una región con mayor densidad de puntos hasta que la convergencia.\n",
    "\n",
    "\n",
    "2. En cada iteración, desplazamos la ventana hacía regiones con mayor densidad de puntos, moviendo el centro a la media de todos los puntos dentro de la ventana (de ahí el nombre). La densidad dentro de nuestra ventana deslizante será proporcional al número de puntos que hay dentro de ella. Y como estamos deslizando el centro hacía la media de la región, también nos estaremos moviendo gradualmente hacía áreas de mayor densidad.\n",
    "\n",
    "\n",
    "3. Continuaremos haciendo este desplazamiento hasta que lleguemos a un punto donde se pierda la concentración de puntos y ya no podamos agregar más a nuestra ventana. En otras palabras, hasta que la densidad de puntos comience a disminuir.\n",
    "\n",
    "\n",
    "4. Repetimos los pasos 1 a 3 inicializando diferentes ventanas deslizantes hasta que todos los puntos que tenemos en nuestro dataset estén dentro de una ventana. Cuando encontremos varias ventanas que se superponen (encierran regiones similares), entonces conservamos la ventana que contiene a la mayoría de los puntos. De esta forma estamos agrupando los datos según las ventanas en las que se encuentran.\n",
    "\n",
    "\n",
    "<div>\n",
    "<img src = \"https://miro.medium.com/max/432/1*vyz94J_76dsVToaa4VG1Zg.gif\" style=\"width:400px;\"/>\n",
    "</div>\n",
    "\n",
    "En contraste con K-Means, **aquí no necesitamos escoger el número de grupos o clases** ya que el algoritmo los descubre automáticamente. Esto nos proporciona una gran ventaja. El hecho de que los centro de los grupos converjan hacía los puntos de densidad máxima es algo muy interesante para nosotros ya que es una manera intuitiva de entender nuestros datos y como están distribuidos. \n",
    "\n",
    "El inconveniente de este algoritmo es que la selección de nuestra relación tamaño / radio de nuestra ventana puede ser no trivial.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Agrupación espacial basada en densidad de aplicaciones con ruido (DBSCAN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este es un algoritmo similar de mean-shift, pero con un par de ventajas considerables. Primero la visualización antes de explicarlo\n",
    "\n",
    "<div>\n",
    "<img src = \"https://miro.medium.com/max/675/1*tc8UF-h0nQqUfLC8-0uInQ.gif\" style=\"width:400px;\"/>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. DBSCAN comienza con un punto arbitrario de nuestro conjunto de datos que no ha sido clasificado. Los puntos vecinos a este son extraídos usando una distancia epsilon $ \\epsilon $ (todos los puntos que estén en una región dada por $\\epsilon$ serán considerados como puntos vecinos).\n",
    "\n",
    "\n",
    "2. Si hay una una cantidad suficiente de puntos (de acuerdo a nuestro parámetro *minPoints*) dentro de esta sección de puntos vecinos entonces, comenzaremos a realizar el proceso de agrupamiento y el punto actual en ese momento pasa a convertirse en el primer punto de un nuevo proceso de agrupación. De lo contrario (no tiene la suficiente cantidad de puntos vecinos) será catalogado como un punto de **ruido**. Ya sea que el punto fue marcado como parte del grupo o como ruido, una vez revisado lo agregamos al grupo y lo marcamos como un punto ya visitado o ya evaluado.\n",
    "\n",
    "\n",
    "3. Para este primer punto en el nuevo cluster o grupo, todos los puntos que se mantengan a una distancia máxima $\\epsilon$ se vuelven parte del mismo cluster. Este procedimiento de convertir todos los puntos cercanos a $\\epsilon$ en puntos del mismo grupo lo repetimos para cada uno de los nuevos puntos que van siendo agregados a nuestro grupo.\n",
    "\n",
    "\n",
    "4. Repetimos el procedimiento de 2 a 3 hasta que todos los puntos de nuestro grupo han sido determinados, es decir, hemos marcado o **etiquetado** como puntos visitados a todos los puntos que se encuentran en las proximidades de distancias $\\epsilon$ de cada uno de nuestros puntos del grupo.\n",
    "\n",
    "\n",
    "5. Una vez que hemos terminado con el cluster o grupo actual, nos movemos a otro punto del set de datos que no ha sido visitado o etiquetado para comenzar un nuevo proceso y obtener un grupo nuevo. Después repetimos hasta que todos y cada uno de nuestros puntos hayan sido \"visitados\". Y una vez hecho esto, tendremos a cada uno de los puntos marcados como pertenecientes a$X$ o $Y$ grupo, o como un punto de ruido dentro del grupo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DBSCAN tiene muchas ventajas sobre otros algoritmos de agrupamiento. Primero, no requiere de la pre-selección de la cantidad de grupos que queremos. Además, identifica aquellos puntos que no se consideran lo suficientemente cercanos como si fueran ruido, cosa que no ocurría en Mean-Shift, donde simplemente los agregaba en un grupo aún si ese punto era muy diferente al resto; Adicionalmente, DBSCAN puede encontrar agrupaciones de cualquier forma y tamaño bastante bien.\n",
    "\n",
    "El principal inconveniente de DBSCAN e que no funciona tan bien como el resto de algoritmos cuando tenemos cluster de diferentes densidades. Esto es una consecuencia del parámetro $\\epsilon$ que nos establece un umbral de distancia y los *minPoints* tendrían que variar de un grupo a otro para detectar diferentes densidades de puntos. Este inconveniente se hace muy presente con datos de muy altas dimensiones, ya que eso hace muy difícil determinar el umbral de distancia $\\epsilon$.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Agrupación de expectativas-maximización (EM) mediante modelos de mezcla gaussiana (GMM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uno de los mayores inconvenientes de K-Means es su uso ingenuo del valor medio para el centro de los grupos. Podemos ver el por qué esta no es la mejor forma de hacerlo al ver el ejemplo de la siguiente imagen:\n",
    "\n",
    "<div>\n",
    "<img src = \"https://miro.medium.com/max/357/1*Xvl-pXxsLAZ7gbTUuvgMtA.png\" style=\"width:400px;\"/>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la imagen de la izquierda es claro para el ojo humano detectar que ahí existen 2 grupos, donde cada grupo es uno de los círculos de diferentes radios que se forman. Aquí K-means nos decepcionaría un poco ya que los valores medios de los grupos están muy cerca entre si. K-means también fallaría para casos donde los grupos no son circulares (por ejemplo, están en forma de línea), de nuevo como resultado de establecer nuestros centros a partir de la media.\n",
    "\n",
    "En el caso de la imagen de la derecha aparece algo muy similar, ya que si bien son dos grupos fácilmente distinguibles para el ojo humano; los datos están muy cercanos entre sí, pero con algunas secciones muy dispersas. Por lo que terminaría por fraccionar nuestros grupos de datos en una relación que no existe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo que hacen los modelos de mezcla gaussiana es, brindarnos mayor flexibilidad que K-Means. Con GMMs podemos asumir que los puntos de datos son una distribución gaussiana o distribución normal.\n",
    "\n",
    "<div>\n",
    "<img src = \"https://2.bp.blogspot.com/-4zOrhNw5Ppg/WJy76ypdv-I/AAAAAAAAZtg/PlxK3HuaYBgJSYrRqYQxMT08lH3cdEfEACLcB/s1600/Distribuci%25C3%25B3n%2BNormal.png\" style=\"width:400px;\"/>\n",
    "</div>\n",
    "\n",
    "Esto es mucho menos restrictivo que suponer que son circulares y utilizar la media. De esta forma tenemos 2 parámetros para describir la forma de nuestros grupos: \n",
    "\n",
    "* La media \n",
    "* La desviación estándar (dispersión respecto a la media).\n",
    "\n",
    "Tomando un ejemplo en 2 dimensiones, estamos diciendo que los grupos o cluster pueden tomar cualquier tipo de forma elíptica (debido a que tenemos la desviación estándar en ambas direcciones $X$ y $Y$). De esta forma, estamos asignando una distribución gaussiana a cada uno de nuestros clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para encontrar los parámetros de la distribución gaussiana de cada cluster, necesitaremos de un algoritmo de optimización llamado **Maximización de expectativas (EM)**. Veamos el ejemplo de la siguiente figura, donde tenemos nuestras distribuciones gaussianas tratando de ajustarse al conjunto de datos:\n",
    "\n",
    "<div>\n",
    "<img src = \"https://miro.medium.com/max/360/1*OyXgise21a23D5JCss8Tlg.gif\n",
    "\" style=\"width:400px;\"/>\n",
    "</div>\n",
    "\n",
    "Ahora podemos continuar con el proceso del agrupamiento por Maximización de expectativas usando modelos de mezcla gaussiana."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Comenzaremos seleccionando el número de clusters (como hacíamos en K-means) e inicializamos aleatoriamente los parámetros de nuestra distribución gaussiana para cada cluster. Podemos intentar inicializarla con buenos parámetros que se aproximen a lo que deseamos echando un pequeño vistazo a los datos. Aunque no es del todo necesario ya que, como se ve en el gif de arriba; los gaussianos comienzan siendo bastante pobres en su certeza y rápidamente se acoplan a los datos.\n",
    "\n",
    "\n",
    "2. Dado las distribuciones gaussianas para cada cluster, calculamos la probabilidad de cada punto de datos pertenezca a cada uno de los clusters. Mientras más cerca este al centro de cierto cluster, entonces es más probable que pertenezca a ese grupo. Esto es bastante intuitivo, ya que estamos asumiendo que la mayoría de datos están cerca del centro de ese cluster.\n",
    "\n",
    "\n",
    "3. Basado en esas probabilidades, calculamos el nuevo set de parámetros para actualizar nuestras distribuciones gaussianas, de forma que busquemos maximizar las probabilidades de que esos puntos estén dentro de los clústers. Calculamos estos nuevos parámetros usando **[sumas ponderadas](https://www.youtube.com/watch?v=eEQe5loN1mQ)** de nuestras posiciones de los datos, donde los pesos son las probabilidades de que los puntos de datos pertenezcan a ese cluster en particular.<br><br> Para explicar esto de una forma más visual, visualicemos el cluster amarillo del gif de arriba. La distribución comienza con parámetros totalmente aleatorios en la primera iteración, pero podemos ver que la mayoría de los puntos del cluster amarillo están del lado derecho de esa distribución. Cuando realizamos el calculo de nuestra suma ponderada de nuestras probabilidades, aunque hay algunos puntos cerca del centro, la mayor parte de ellos están hacía la derecha. Así que, naturalmente, la media de nuestra distribución se acerca más a ese conjunto de puntos.<br><br> Podemos ver también que la mayor parte de los puntos de ese cluster están desde la parte superior derecha hacía la parte inferior izquierda. Por lo tando, la desviación estándar cambia para crear una elipse más ajustada a estos puntos, maximizando la suma ponderada por las probabilidades.\n",
    "\n",
    "\n",
    "4. Repetimos los pasos 2 y 3 de forma iterativa hasta la convergencia, donde las distribuciones no cambian mucho de una iteración a otra (Los cambios dejan de ser significantes para el costo computacional).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aquí hay dos ventajas clave al utilizar los algoritmos basados en GMMs. Primero, los GGMs son **mucho más flexibles** en términos de [covarianza](https://es.wikipedia.org/wiki/Covarianza) de clusters que en K-Means; debido a nuestra desviación estándar, los clusters pueden asemejar a elipses de cualquier forma, en lugar de ser restringidas a círculos. \n",
    "\n",
    "K-Means se puede ver como un caso específico de GMM en el cuál, la covarianza de cada cluster a través de todas las dimensiones tiende a 0. Además, dado que GMMs utiliza probabilidades, puede tener distintos clusters para cada punto de datos. Entonces, si un punto de datos esta en medio de 2 clusters que se superponen, podemos simplificarlo diciendo que pertenece un $X\\%$ al cluster $A$ y un $Y\\%$ al cluster $B$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Agrupación jerárquica aglomerativa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este tipo de algoritmo tiene 2 categorías:\n",
    "\n",
    "* Top-Down (ascendentes)\n",
    "* Bottom-up (descendentes)\n",
    "\n",
    "El primero, Top-Down o ascendentes trata a cada uno de los datos como un cluster individual desde el inicio y luego, sucesivamente, junta o aglomera pares de clusters hasta que todos los clusters han sido fusionados en un solo cluster que contiene a todos los puntos de datos.\n",
    "\n",
    "El Bottom-up o descendente jerárquico, también llamado **agrupamiento aglomerativo jerárquico o HAC** es un método de agrupamiento que se representa como un árbol (o dendrograma). La raíz de nuestro árbol es el único cluster que reúne todas las muestras, lo que serían como nuestras hojas son nueestras muestras.\n",
    "\n",
    "Veamos una visualización de estas categorías de agrupación jerárquica:\n",
    "\n",
    "<div>\n",
    "<img src = \"https://miro.medium.com/max/700/1*ET8kCcPpr893vNZFs8j4xg.gif\" style=\"width:400px;\"/>\n",
    "</div>\n",
    "\n",
    "<br>\n",
    "\n",
    "<div>\n",
    "<img src = \"https://www.statisticshowto.com/wp-content/uploads/2016/11/clustergram.png\" style=\"width:400px;\"/>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos los pasos que realiza este algoritmo:\n",
    "\n",
    "1. Comenzamos tratando a cada punto de datos como un cluster individual, de forma que si tenemos $X$ puntos, entonces tendremos $X$ clusters. Luego seleccionamos una métrica de distancia que mida la distancia entre 2 clusters. Como ejemplo, usaremos una **vinculación promedio** que define la distancia entre dos clusters como la distancia promedio entre los puntos dados en el primer y segundo cluster (o datos).\n",
    "\n",
    "\n",
    "2. En cada iteración, combinaremos 2 clusters en uno solo. Los dos clusters combinados son seleccionados como aquellos con el vinculo o semejanza promedio más pequeña. Por ejemplo, de acuerdo con nuestra métrica de distancia seleccionada, estos dos clusters tienen la distancia más pequeña entre sí y por lo tanto, son semejantes y deben formar parte del mismo cluster.\n",
    "\n",
    "3. Repetimos el paso 2 hasta que lleguemos a la raíz de nuestro árbol, es decir, ahora contamos con un solo cluster que engloba a todos los datos que tenemos. De esta forma, podemos seleccionar cuántos clusters queremos al final, simplemente eligiendo cuando parar la combinación de clusters, es decir, cuando dejemos de construir el árbol."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este método de agrupamiento no requiere que le demos un número específico de clusters ya que cada elemento o dato es un cluster pos si mismo, incluso podemos decidir hasta que punto combinarlos para crear nuestro árbol. Adicionalmente, **el algoritmo no es sensible a nuestra métrica de distancia**; todos tienden a funcionar igual de bien, mientras que en otros algoritmos de agrupamiento, esta decisión es realmente importante e influye en el resultado final. <br><br> *Un uso particular en el que destaca el uso de este algoritmo es cuando los datos tienen ya una estructura subyacente que no conocemos del todo, pero queremos recuperarla o traerla a la luz de nuevo*; otros algoritmos de agrupamiento no tienen esta propiedad, pero no todo es genial aquí, *ya que todas estas ventajas que nos brindan los algoritmos de agrupación jerárquica lo hacen a costa de ser menos eficientes*. Estos algoritmos tienen una complejidad de $O(n^3)$, una gran diferencia respecto a K-Means y GMM que tienen un $O(n)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como un pequeño aporte de Scikit Learn, aquí una imagen que representa visualmente como cada uno de los principales algoritmos de agrupación procesan distintos datasets de entrada.\n",
    "\n",
    "<div>\n",
    "<img src = \"https://miro.medium.com/max/700/1*oNt9G9UpVhtyFLDBwEMf8Q.png\"/>\n",
    "</div>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fuentes: \n",
    "\n",
    "[The 5 Clustering Algorithms Data Scientists Need to Know](https://towardsdatascience.com/the-5-clustering-algorithms-data-scientists-need-to-know-a36d136ef68)\n",
    "\n",
    "[Clustering Algorithms](https://developers.google.com/machine-learning/clustering/clustering-algorithms)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agrupamiento jerárquico"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " * Es un algoritmo que agrupa objetos similares en grupos llamados crusters.\n",
    "* El algoritmo comienza tratando a cada objeto como un cluster individual y luego realiza los siguientes pasos de manera recursiva:\n",
    "    * Identifica los dos clusters con menos distancia entre sí.\n",
    "    * Agrupa esos clusters en un cluster nuevo.\n",
    "* El output final es un dendrograma que muestra la relación entre objetos y grupos.\n",
    "    \n",
    "    <div>\n",
    "<img src = \"https://support.minitab.com/es-mx/minitab/18/cluster_obs_dendrogram_with_final_partition_glove_testers.png\"/>\n",
    "</div>\n",
    "\n",
    "* Es importante determinar qué medida o métrica de distancia vamos a utilizar y los puntos a utilizar en cada cluster (linkage criteria)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tratemos de hacer nuestra propia implementación y luego revisemos como esta incorporado en librerías que ya tienen una función para realizar esa tarea:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ingrese la cantidad de datos a usar :5\n",
      "Todos los puntos: [[7, 9], [1, 5], [1, 7], [0, 7], [2, 7]]\n",
      "\n",
      "Dato actual: [7, 9] \n",
      "\n",
      "Distancia al punto [1, 5]= 7.211 unidades\n",
      "Distancia al punto [1, 7]= 6.325 unidades\n",
      "Distancia al punto [0, 7]= 7.28 unidades\n",
      "Distancia al punto [2, 7]= 5.385 unidades\n",
      "\n",
      "Distancias: [7.211, 6.325, 7.28, 5.385]\n",
      "Valor mínimo: 5.385\n",
      "\n",
      "El valor mínimo es el indice 3 de la lista de distancias,\n",
      "que corresponde al punto [2, 7].\n",
      "\n",
      "Punto actual: [7, 9]\n",
      "Punto cercano: [2, 7]\n",
      "Radio: 2.6925\n",
      "Centro: 7, 9\n",
      "Nuevo cluster: [4.5, 8.0]\n",
      "--------------------------------\n",
      "Clusters:[[4.5, 8.0]]\n",
      "Todos los puntos: [[1, 5], [1, 7], [0, 7]]\n",
      "\n",
      "Dato actual: [4.5, 8.0] \n",
      "\n",
      "Distancia al punto [1, 5]= 4.61 unidades\n",
      "Distancia al punto [1, 7]= 3.64 unidades\n",
      "Distancia al punto [0, 7]= 4.61 unidades\n",
      "\n",
      "Distancias: [4.61, 3.64, 4.61]\n",
      "Valor mínimo: 3.64\n",
      "\n",
      "El valor mínimo es el indice 1 de la lista de distancias,\n",
      "que corresponde al punto [1, 7].\n",
      "\n",
      "Punto actual: [4.5, 8.0]\n",
      "Punto cercano: [1, 7]\n",
      "Nuevo cluster: [2.75, 7.5]\n",
      "--------------------------------\n",
      "Clusters:[[4.5, 8.0], [2.75, 7.5]]\n",
      "Todos los puntos: [[1, 5], [0, 7]]\n",
      "\n",
      "Dato actual: [2.75, 7.5] \n",
      "\n",
      "Distancia al punto [1, 5]= 3.052 unidades\n",
      "Distancia al punto [0, 7]= 2.795 unidades\n",
      "\n",
      "Distancias: [3.052, 2.795]\n",
      "Valor mínimo: 2.795\n",
      "\n",
      "El valor mínimo es el indice 1 de la lista de distancias,\n",
      "que corresponde al punto [0, 7].\n",
      "\n",
      "Punto actual: [2.75, 7.5]\n",
      "Punto cercano: [0, 7]\n",
      "Nuevo cluster: [1.375, 7.25]\n",
      "--------------------------------\n",
      "Clusters:[[4.5, 8.0], [2.75, 7.5], [1.375, 7.25]]\n",
      "Todos los puntos: [[1, 5]]\n",
      "\n",
      "Dato actual: [1.375, 7.25] \n",
      "\n",
      "Distancia al punto [1, 5]= 2.281 unidades\n",
      "\n",
      "Distancias: [2.281]\n",
      "Valor mínimo: 2.281\n",
      "\n",
      "El valor mínimo es el indice 0 de la lista de distancias,\n",
      "que corresponde al punto [1, 5].\n",
      "\n",
      "Punto actual: [1.375, 7.25]\n",
      "Punto cercano: [1, 5]\n",
      "Nuevo cluster: [1.1875, 6.125]\n",
      "--------------------------------\n",
      "Clusters:[[4.5, 8.0], [2.75, 7.5], [1.375, 7.25], [1.1875, 6.125]]\n"
     ]
    }
   ],
   "source": [
    "# py -m venv env\n",
    "# env\\Scripts\\activate.bat\n",
    "import random \n",
    "from random import randint\n",
    "import math\n",
    "from bokeh.layouts import row\n",
    "from bokeh.plotting import figure, show\n",
    "\n",
    "from bokeh.models import LabelSet, ColumnDataSource\n",
    "from bokeh.palettes import Category20 as palette\n",
    "\n",
    "\n",
    "def crear_datos():#Creamos los puntitos que representarán el dataset\n",
    "    datos = []\n",
    "    # Generamos unos datos aleatorios\n",
    "    for _ in range (numero_de_datos):\n",
    "        x = random.randint(0,10)\n",
    "        y = random.randint(0,10)\n",
    "        datos.append([x,y])\n",
    "    #Revisamos cuales son\n",
    "    #print(f'Todos los puntos: {datos}')\n",
    "    return datos\n",
    "\n",
    "def calcula_y_dibuja(datos): #Calcular distancia entre puntos M#1\n",
    "    #Seleccionamos el punto 0 para iniciar el arbol\n",
    "    #Como son aleatorios, da igual cual escogemos, pero facilita\n",
    "    #el proceso ya que solo falta recorrer los elementos de la lista.\n",
    "    \n",
    "    x_datos = []\n",
    "    y_datos = []\n",
    "    x_clusters = []\n",
    "    y_clusters = []\n",
    "    \n",
    "    for i in datos:\n",
    "        #print(i[0])\n",
    "        x_datos.append(i[0])\n",
    "    #print(f'X: {x_datos}')\n",
    "    \n",
    "    for i in datos:\n",
    "        #print(i[1])\n",
    "        y_datos.append(i[1])\n",
    "    #print(f'Y: {y_datos}')\n",
    "    \n",
    "    #create a new plot\n",
    "    p = figure(\n",
    "        title=\"Clusterización jerárquica\",\n",
    "        sizing_mode=\"stretch_width\",\n",
    "        plot_width=800,\n",
    "        plot_height=800,\n",
    "                    )\n",
    "\n",
    "\n",
    "    # add circle renderer with legend_label arguments\n",
    "    circle = p.circle(\n",
    "        x_datos,\n",
    "        y_datos,\n",
    "        legend_label=\"Datos originales\",\n",
    "        fill_color= \"#000000\",\n",
    "        fill_alpha=10,\n",
    "        line_color= \"#000000\",\n",
    "        size=10,\n",
    "                        )\n",
    "\n",
    "    # display legend in top left corner (default is top right corner)\n",
    "    p.legend.location = \"top_right\"\n",
    "\n",
    "    # add a title to your legend\n",
    "    p.legend.title = \"Información de puntitos\"\n",
    "\n",
    "    # change appearance of legend text\n",
    "    p.legend.label_text_font = \"times\"\n",
    "    p.legend.label_text_font_style = \"italic\"\n",
    "    p.legend.label_text_color = \"navy\"\n",
    "\n",
    "    # change border and background of legend\n",
    "    p.legend.border_line_width = 3\n",
    "    p.legend.border_line_color = \"#21B4B3\"\n",
    "    p.legend.border_line_alpha = 0.8\n",
    "    p.legend.background_fill_color = \"#092D4D\"\n",
    "    p.legend.background_fill_alpha = 0.2\n",
    "    \n",
    "    \n",
    "    actual = datos[0] \n",
    "    while len(datos) > 0:   \n",
    "        distancias = []\n",
    "        if actual == datos[0]: \n",
    "            print(f'Todos los puntos: {datos}')\n",
    "            print(f'\\nDato actual: {actual} \\n')\n",
    "            clusters = []\n",
    "            for i in datos[1::]:      \n",
    "                #####Color hexadecimal aleatorio###########\n",
    "                hex_digits = ['0','1','2','3','4','5','6','7','8','9','a','b','c','d','e','f']\n",
    "                digit_array = []\n",
    "                for j in range(6):\n",
    "                    digit_array.append(hex_digits[randint(0,15)])\n",
    "                joined_digits = ''.join(digit_array)\n",
    "                hex_number = '#' + joined_digits\n",
    "                ############################################ \n",
    "                \n",
    "                dis_x = abs((actual[0])-(i[0]))\n",
    "                dis_y = abs((actual[1])-(i[1]))\n",
    "                distancia = abs(round((math.sqrt((dis_x)**2+(dis_y)**2)),3))\n",
    "                #distancias.append(distancia)\n",
    "                print(f'Distancia al punto {i}= {distancia} unidades')\n",
    "\n",
    "                distancias.append(distancia)\n",
    "                \n",
    "            print(f'\\nDistancias: {distancias}')\n",
    "            minimo = min(distancias)\n",
    "            print(f'Valor mínimo: {minimo}')\n",
    "            index = distancias.index(minimo)\n",
    "            print(f'\\nEl valor mínimo es el indice {index} de la lista de distancias,\\nque corresponde al punto {datos[index+1]}.\\n')\n",
    "            print(f'Punto actual: {actual}')\n",
    "            print(f'Punto cercano: {datos[index+1]}')\n",
    "            print(f'Radio: {minimo/2}')\n",
    "            print(f'Centro: {actual[0]}, {actual[1]}')\n",
    "\n",
    "            actual = [(actual[0] + datos[index+1][0] ) / 2, (actual[1] + datos[index+1][1] ) / 2]\n",
    "            print(f'Nuevo cluster: {actual}')\n",
    "            ####Dibujar Clusters#############\n",
    "            circle_clusters = p.circle(\n",
    "                actual[0],\n",
    "                actual[1],\n",
    "                legend_label=\"1er cluster\",\n",
    "                fill_color=hex_number,\n",
    "                fill_alpha=0.1,\n",
    "                line_color=hex_number,\n",
    "                radius=minimo/2,                   )\n",
    "            #########################\n",
    "            clusters.append(actual)\n",
    "            #############################\n",
    "            print('--------------------------------')\n",
    "            datos.pop(0)\n",
    "            datos.pop(index)\n",
    "\n",
    "        else:\n",
    "            \n",
    "            print(f'Todos los puntos: {datos}')\n",
    "            print(f'\\nDato actual: {actual} \\n')\n",
    "            for i in datos[::]:\n",
    "                #####Color hexadecimal aleatorio###########\n",
    "                hex_digits = ['0','1','2','3','4','5','6','7','8','9','a','b','c','d','e','f']\n",
    "                digit_array = []\n",
    "                for j in range(6):\n",
    "                    digit_array.append(hex_digits[randint(0,15)])\n",
    "                joined_digits = ''.join(digit_array)\n",
    "                hex_number = '#' + joined_digits\n",
    "                ############################################ \n",
    "                dis_x = abs((actual[0])-(i[0]))\n",
    "                dis_y = abs((actual[1])-(i[1]))\n",
    "                distancia = abs(round((math.sqrt((dis_x)**2+(dis_y)**2)),3))\n",
    "                #distancias.append(distancia)\n",
    "\n",
    "                distancias.append(distancia)\n",
    "\n",
    "                print(f'Distancia al punto {i}= {distancia} unidades')\n",
    "            print(f'\\nDistancias: {distancias}')\n",
    "            minimo = min(distancias)\n",
    "            print(f'Valor mínimo: {minimo}')\n",
    "            index = distancias.index(minimo)\n",
    "            print(f'\\nEl valor mínimo es el indice {index} de la lista de distancias,\\nque corresponde al punto {datos[index]}.\\n')\n",
    "            print(f'Punto actual: {actual}')\n",
    "            print(f'Punto cercano: {datos[index]}')\n",
    "            actual = [(actual[0] + datos[index][0] ) / 2, (actual[1] + datos[index][1] ) / 2]\n",
    "            print(f'Nuevo cluster: {actual}')\n",
    "            \n",
    "            ####Dibujar Clusters#############\n",
    "            circle_clusters = p.circle(\n",
    "                actual[0],\n",
    "                actual[1],\n",
    "                legend_label=\"Resto de clusters\",\n",
    "                fill_color=hex_number,\n",
    "                fill_alpha=0.1,\n",
    "                line_color=hex_number,\n",
    "                radius=minimo/2,\n",
    "                                    )\n",
    "                ###############################\n",
    "            \n",
    "\n",
    "            \n",
    "            #########################\n",
    "            clusters.append(actual)\n",
    "            #############################\n",
    "            print('--------------------------------')\n",
    "            datos.pop(index)\n",
    "            \n",
    "        ##################\n",
    "        print(f'Clusters:{clusters}')     \n",
    "\n",
    "    show(p)\n",
    "\n",
    "\n",
    "    # show the results\n",
    "    \n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    numero_de_datos = int(input('Ingrese la cantidad de datos a usar :'))\n",
    "    datos = crear_datos()\n",
    "    clusters = calcula_y_dibuja(datos)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agrupamiento por K-means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Es un algoritmo que agrupa utilizando centroides.\n",
    "\n",
    "* El algoritmo funciona asignando puntos al azar (K define el número inicial de clusters) y después:\n",
    "\n",
    "    * En cada iteración el punto se ajusta a su nuevo centroide y cada punto se recalcula con la distancia con respecto de los centroides.\n",
    "    \n",
    "    * Los puntos se reasignan al nuevo centro.\n",
    "    \n",
    "    * El algoritmo se repite de manera iterativa hasta que ya no existen mejoras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://upload.wikimedia.org/wikipedia/commons/thumb/e/ea/K-means_convergence.gif/512px-K-means_convergence.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para este algoritmo debemos de tener en cuenta varias consideraciones. Primero, debemos de elegir un $K$ (número de grupos) adecuado para nuestro set de datos. Por lo que es bueno tener conocimiento previo sobre el problema y el dominio de los datos para tener una intuición que nos ayude a seleccionar un $K$ adecuado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Además, computacionalmente es muy pesado. Por lo que, para data sets muy extensos, sería mejor idea realizar un muestreo representativo de los grupos y trabajar con ellos en lugar de trabajar con todos los datos a la vez.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementación del algoritmo K-means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para realizar la implementación vamos a enlistar los pasos necesarios para llevar a cabo el proceso:\n",
    "\n",
    "1. Seleccionar el número de k grupos (clusters)\n",
    "2. Generar aleatoriamente k puntos que llamaremos centroides\n",
    "3. Asignar cada elemento del conjunto de datos al centroide más cercano para formar k grupos\n",
    "4. Reasignar la posición de cada centroide\n",
    "5. Reasignar los elementos de datos al centroide más cercano nuevamente\n",
    "   * Si hubo elementos que se asignaron a un centroide distinto al original, regresar al paso 4, de lo contrario, el proceso ha terminado\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¿De que tamaño será el dataset?: 5\n",
      "¿Cuantos grupos se obtendrán (K)?: 3\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datos que recibe: [[95, 83], [38, 34], [31, 12], [93, 59], [22, 58]]\n",
      "Clusters: [[62, 33], [44, 4], [73, 46]]\n",
      "X: [[[62]], [[44]], [[73]]]\n",
      "Y: [[[33]], [[4]], [[46]]]\n",
      "Distancias: [59.908, 94.032, 43.046]\n",
      "Distancia minima: 43.046\n",
      "El punto [95, 83] tiene asignado al centroide [73, 46]\n",
      "\n",
      "Punto X = 95 , Centoride X = 73 \n",
      "Punto Y = 83 , Centroide Y = 46 \n",
      "X : [[[62]], [[44]], [[73]]] , Y: [[[33]], [[4]], [[46]]]\n",
      "X : [[[62]], [[44]], [[73], [95]]] , Y: [[[33]], [[4]], [[46], [83]]]\n",
      "------------------------------------\n",
      "Distancias: [24.021, 30.594, 37.0]\n",
      "Distancia minima: 24.021\n",
      "El punto [38, 34] tiene asignado al centroide [62, 33]\n",
      "\n",
      "Punto X = 38 , Centoride X = 62 \n",
      "Punto Y = 34 , Centroide Y = 33 \n",
      "X : [[[62]], [[44]], [[73], [95]]] , Y: [[[33]], [[4]], [[46], [83]]]\n",
      "X : [[[62], [38]], [[44]], [[73], [95]]] , Y: [[[33], [34]], [[4]], [[46], [83]]]\n",
      "------------------------------------\n",
      "Distancias: [37.443, 15.264, 54.037]\n",
      "Distancia minima: 15.264\n",
      "El punto [31, 12] tiene asignado al centroide [44, 4]\n",
      "\n",
      "Punto X = 31 , Centoride X = 44 \n",
      "Punto Y = 12 , Centroide Y = 4 \n",
      "X : [[[62], [38]], [[44]], [[73], [95]]] , Y: [[[33], [34]], [[4]], [[46], [83]]]\n",
      "X : [[[62], [38]], [[44], [31]], [[73], [95]]] , Y: [[[33], [34]], [[4], [12]], [[46], [83]]]\n",
      "------------------------------------\n",
      "Distancias: [40.46, 73.661, 23.854]\n",
      "Distancia minima: 23.854\n",
      "El punto [93, 59] tiene asignado al centroide [73, 46]\n",
      "\n",
      "Punto X = 93 , Centoride X = 73 \n",
      "Punto Y = 59 , Centroide Y = 46 \n",
      "X : [[[62], [38]], [[44], [31]], [[73], [95]]] , Y: [[[33], [34]], [[4], [12]], [[46], [83]]]\n",
      "X : [[[62], [38]], [[44], [31]], [[73], [95], [93]]] , Y: [[[33], [34]], [[4], [12]], [[46], [83], [59]]]\n",
      "------------------------------------\n",
      "Distancias: [47.17, 58.31, 52.393]\n",
      "Distancia minima: 47.17\n",
      "El punto [22, 58] tiene asignado al centroide [62, 33]\n",
      "\n",
      "Punto X = 22 , Centoride X = 62 \n",
      "Punto Y = 58 , Centroide Y = 33 \n",
      "X : [[[62], [38]], [[44], [31]], [[73], [95], [93]]] , Y: [[[33], [34]], [[4], [12]], [[46], [83], [59]]]\n",
      "X : [[[62], [38], [22]], [[44], [31]], [[73], [95], [93]]] , Y: [[[33], [34], [58]], [[4], [12]], [[46], [83], [59]]]\n",
      "------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 720x720 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import random \n",
    "from random import randint\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def crear_datos_y_ks(numero_de_datos, k):\n",
    "    # Generamos un dataset de datos aleatorios\n",
    "    # Y nuestras K's en puntos aleatorios también\n",
    "    datos = []\n",
    "    ks = []\n",
    "    for _ in range (numero_de_datos):\n",
    "        x = random.randint(0,100)\n",
    "        y = random.randint(0,100)\n",
    "        datos.append([x,y])\n",
    "    for _ in range (k):\n",
    "        k_x = random.randint(0,100)\n",
    "        k_y = random.randint(0,100)\n",
    "        ks.append([k_x,k_y])\n",
    "    return datos,ks\n",
    "\n",
    "\n",
    "def dibujar(datos_y_k):\n",
    "    #print(f'\\nEstos son los datos que recibe: {datos_y_k[0]}')\n",
    "    #print(f'Estos son los K puntos que recibe: {datos_y_k[1]}\\n')\n",
    "    # Coordenadas\n",
    "    x = []\n",
    "    for i in datos_y_k[0]:\n",
    "        x.append(i[0])\n",
    "    y = []\n",
    "    for i in datos_y_k[0]:\n",
    "        y.append(i[1])\n",
    "    # K puntos\n",
    "    k_x = []\n",
    "    for i in datos_y_k[1]:\n",
    "        k_x.append(i[0])\n",
    "    k_y = []\n",
    "    for i in datos_y_k[1]:\n",
    "        k_y.append(i[1])\n",
    "    # Dibuja\n",
    "    fig, ax = plt.subplots()\n",
    "    plt.ylabel('Y')\n",
    "    plt.xlabel('X')\n",
    "    plt.plot(x, y, 'o', color='#000000')\n",
    "    plt.plot(k_x, k_y, 'P')\n",
    "    plt.show()\n",
    "    return datos_y_k[0], datos_y_k[1]\n",
    "    \n",
    "\n",
    "def centroide_mas_cercano(coordenadas):\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    fig, ax = plt.subplots()\n",
    "    plt.ylabel('Y')\n",
    "    plt.xlabel('X')\n",
    "    plt.suptitle('Primera iteración')\n",
    "    print(f'Datos que recibe: {coordenadas[0]}')\n",
    "\n",
    "    print(f'Clusters: {coordenadas[1]}')\n",
    "    \n",
    "    x_datos = []\n",
    "    for i in coordenadas[1]:\n",
    "        x_datos.append([[i[0]]])\n",
    "    print(f'X: {x_datos}')   \n",
    "    \n",
    "    y_datos = []\n",
    "    for i in coordenadas[1]:\n",
    "        y_datos.append([[i[1]]])\n",
    "    print(f'Y: {y_datos}')\n",
    "    \n",
    "    for i in coordenadas[0]:\n",
    "        xs = []\n",
    "        ys = []\n",
    "        #####Color hexadecimal aleatorio###########\n",
    "        hex_digits = ['0','1','2','3','4','5','6','7','8','9','a','b','c','d','e','f']\n",
    "        digit_array = []\n",
    "        for j in range(6):\n",
    "            digit_array.append(hex_digits[randint(0,15)])\n",
    "        joined_digits = ''.join(digit_array)\n",
    "        hex_number = '#' + joined_digits\n",
    "        ############################################ \n",
    "        distancias = []\n",
    "        for j in coordenadas[1]:\n",
    "            dis_x = abs((i[0])-(j[0]))\n",
    "            dis_y = abs((i[1])-(j[1]))\n",
    "            distancia = abs(round((math.sqrt((dis_x)**2+(dis_y)**2)),3))\n",
    "            #print(distancia)\n",
    "            distancias.append(distancia)\n",
    "            minimo = min(distancias)\n",
    "            #print(minimo)\n",
    "        print(f'Distancias: {distancias}')\n",
    "        minimo = min(distancias)\n",
    "        print(f'Distancia minima: {minimo}')\n",
    "        index = distancias.index(minimo)\n",
    "        print(f'El punto {i} tiene asignado al centroide {coordenadas[1][index]}\\n')\n",
    "        # Dibuja los puntos individuales\n",
    "        plt.plot(i[0], i[1], 'o', color='#000000')\n",
    "        plt.plot(coordenadas[1][index][0], coordenadas[1][index][1], marker='P')\n",
    "        #Traza la linea\n",
    "        plt.plot([i[0],coordenadas[1][index][0]], [ i[1],coordenadas[1][index][1] ], color=hex_number )\n",
    "        print(f'Punto X = {i[0]} , Centoride X = {coordenadas[1][index][0]} ')\n",
    "        print(f'Punto Y = {i[1]} , Centroide Y = {coordenadas[1][index][1]} ')      \n",
    "        ################################################################### \n",
    "        print(f'X : {x_datos} , Y: {y_datos}')\n",
    "        x_datos[index].append([ i[0] ])\n",
    "        y_datos[index].append([ i[1] ])\n",
    "        print(f'X : {x_datos} , Y: {y_datos}')\n",
    "        print('------------------------------------')\n",
    "        \n",
    "    plt.show()\n",
    "    return x_datos, y_datos\n",
    "            \n",
    "if __name__ == '__main__':\n",
    "    numero_de_datos = int(input('¿De que tamaño será el dataset?: '))\n",
    "    k = int(input(\"¿Cuantos grupos se obtendrán (K)?: \"))\n",
    "    datos_y_k = crear_datos_y_ks(numero_de_datos, k)\n",
    "    coordenadas = dibujar(datos_y_k)\n",
    "    centroide_mas_cercano(coordenadas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fuentes adicionales:\n",
    "\n",
    "[ALGORITMOS DE APRENDIZAJE: KNN & KMEANS](http://www.it.uc3m.es/~jvillena/irc/practicas/08-09/06.pdf)\n",
    "\n",
    "[k-Means Clustering con Python\n",
    "](https://www.jacobsoft.com.mx/es_mx/k-means-clustering-con-python/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": "",
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Tabla de contenidos",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "125.458px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
