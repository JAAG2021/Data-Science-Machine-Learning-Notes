{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Tabla de contenido 游눞<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Matem치ticas-para-Data-Science:-Probabilidad\" data-toc-modified-id=\"Matem치ticas-para-Data-Science:-Probabilidad-1\">Matem치ticas para Data Science: Probabilidad</a></span><ul class=\"toc-item\"><li><span><a href=\"#Incertidumbre-y-probabilidad\" data-toc-modified-id=\"Incertidumbre-y-probabilidad-1.1\">Incertidumbre y probabilidad</a></span><ul class=\"toc-item\"><li><span><a href=\"#Axiomas-de-la-probabilidad\" data-toc-modified-id=\"Axiomas-de-la-probabilidad-1.1.1\">Axiomas de la probabilidad</a></span><ul class=\"toc-item\"><li><span><a href=\"#Axioma-1.\" data-toc-modified-id=\"Axioma-1.-1.1.1.1\">Axioma 1.</a></span></li><li><span><a href=\"#Axioma-2.\" data-toc-modified-id=\"Axioma-2.-1.1.1.2\">Axioma 2.</a></span></li><li><span><a href=\"#Axioma-3.\" data-toc-modified-id=\"Axioma-3.-1.1.1.3\">Axioma 3.</a></span></li></ul></li><li><span><a href=\"#Propiedades-que-se-deducen-de-los-axiomas\" data-toc-modified-id=\"Propiedades-que-se-deducen-de-los-axiomas-1.1.2\">Propiedades que se deducen de los axiomas</a></span></li><li><span><a href=\"#Probabilidad-en-Machine-Learning\" data-toc-modified-id=\"Probabilidad-en-Machine-Learning-1.1.3\">Probabilidad en Machine Learning</a></span><ul class=\"toc-item\"><li><span><a href=\"#Fuentes-de-incertidumbre\" data-toc-modified-id=\"Fuentes-de-incertidumbre-1.1.3.1\">Fuentes de incertidumbre</a></span></li></ul></li></ul></li><li><span><a href=\"#Fundamentos-de-probabilidad\" data-toc-modified-id=\"Fundamentos-de-probabilidad-1.2\">Fundamentos de probabilidad</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matem치ticas para Data Science: Probabilidad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Incertidumbre y probabilidad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Axiomas de la probabilidad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dado un conjunto de sucesos elementales, $\\Omega$, sobre el que se ha definido una $\\sigma$-치lgebra (familia $\\Sigma$ no vac칤a de subconjuntos de un conjunto $X$, cerrada bajo complementos, uniones e intersecciones contables) $\\sigma$ de conjuntos de $\\Omega$ y una funci칩n $P$ que asigna valores reales a los miembros de $\\sigma$, a los que denominamos \"sucesos\", se dice que $P$ es una probabilidad sobre $(\\Omega, \\sigma)$ si se cumplen los siguientes 3 axiomas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Axioma 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La probabilidad de un evento $S$ no puede ser negativa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$0 \\leq P(S)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Axioma 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La probabilidad de un evento seguro, $\\Omega$, es igual a 1, denotado simb칩licamente como:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$P(\\Omega) = 1$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Axioma 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si $E_1, E_2, ...$ son eventos **mutuamente excluyentes** (su intersecci칩n es el conjunto vac칤o), entonces:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ P(E_1 \\cup E_2 \\cup ...) = \\sum{P(E_i)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seg칰n este axioma se puede calcular la probabilidad de un suceso compuesto de varias alternativas mutuamente excluyentes sumando las probabilidades de sus componentes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En t칠rminos m치s formales, una probabilidad es una medida sobre una $\\sigma$-치lgebra (sigma-치lgebra) de subconjuntos del espacio muestral, siendo los subconjuntos miembros de la $\\sigma$-치lgebra los sucesos y definida de tal manera que la medida del total sea 1. Tal medida, gracias a su definici칩n matem치tica, verifica igualmente los 3 axiomas de Kolmog칩rov."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A la terna formada por el **espacio muestral, la $\\sigma$-치lgebra y la funci칩n de probabilidad** se la denomina **espacio probabil칤stico** , esto es, un \"espacio de sucesos\" en el que se ha definido los posibles sucesos a considerar (la $ \\sigma$-치lgebra) y la probabilidad de cada suceso (la funci칩n de probabilidad)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Propiedades que se deducen de los axiomas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De los axiomas anteriores podemos obtener las siguiente proposiciones:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. $P(\\phi)= 0$ donde el conjunto vac칤o ($\\phi$) representa en probabilidad el **suceso imposible**.\n",
    "\n",
    "2. Para cualquier evento, $P(E) \\leq 1$.\n",
    "\n",
    "3. $P(A^c) = 1 - P(A)$, donde $A^c$ representa el conjunto complemento de $A$ (todos los elementos que no est치n en $A$).\n",
    "\n",
    "4. Si $E \\subseteq F$ entonces, $P(E) \\leq P(F)$. Si $E$ es un subconjunto de $F$, la probabilidad de $E$ es menor que la probabilidad de $F$.\n",
    "\n",
    "5. $P(E \\cup F) = P(E) + P(F) - P(E \\cap F)$. Sumamos las probabilidades individuales y restamos la parte en que interseccionan para no contarla 2 veces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probabilidad en Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fuentes de incertidumbre"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Datos**: Para obtener los datos partimos de una obtenci칩n o una medici칩n de datos, por lo que esta recolecci칩n de datos siempre estar치 sujeta a ciertos m치rgenes de error que mantendr치n nuestros datos desde un inicio con cierta \"imperfecci칩n\" o incertidumbre."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Atributos del modelo**: Tambi칠n llamados **predictores** son un subconjunto o reducci칩n de todas las variables que intervienen en un problema en espec칤fico para facilitar la resoluci칩n del problema. Es decir, al estudiar un problema complejo usualmente estudiamos ciertas variables que resultan relevantes para nosotros o que creemos que est치n estrechamente relacionadas con el fen칩meno de estudio, pero en realidad puede que estemos dejando muchas variables de lado, las cuales pueden hacer peque침as contribuciones al problema real."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Arquitectura del modelo**: Nuestro modelo al final de cuentas siempre ser치 una versi칩n simplificada de la realidad, de forma que podamos \"aproximar\" el comportamiento que estamos interesados en estudiar. As칤 que es de esperarse que se \"pierda\" algo de informaci칩n o exactitud cuando comparemos nuestros resultados arrojados por el modelo vs el fen칩meno real. Nuestro trabajo es tratar de aproximarnos lo m치s posible y minimizar esos errores con modelos m치s completos y complejos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por ejemplo, al trabajar con modelos de clasificaci칩n trabajamos con probabilidades para deducir a que grupo pertenece cierto elemento de entrada. Aunque nunca tendremos una certeza del 100% si que podremos calcular las probabilidades individuales de que pertenezca a cada uno de los grupos para tomar decisiones m치s certeras ya que nuestro modelo nos arrojar치 la opci칩n que nos arroje las probabilidades m치s altas. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos un ejemplo de la arquitectura general de un modelo de clasificaci칩n supervisada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://i.imgur.com/sxCngb6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero tenemos nuestra **fase de entrenamiento** donde comenzamos con nuestros datos etiquetados, de forma que tenemos ya ubicados ciertos atributos o features para despu칠s pasar por un extractor de atributos de forma que ubica las variables m치s importantes para que el modelo realice sus predicciones. Una vez pasa por el extractor pasamos de tener un input de cierto tipo a tener 칰nicamente atributos o variables con las que vamos a trabajar. Posteriormente entra en juego nuestro algoritmos de Machine Learning, que variar치n dependiendo de la tarea a resolver y que t칠cnicas va a utilizar, pero al final va a arrojarnos un modelo matem치tico de clasificaci칩n que se adaptar치 seg칰n nuestro data set de entrenamiento.\n",
    "\n",
    "Despu칠s, en nuestra parte de **predicci칩n** una vez que ya tenemos nuestro modelo de clasificaci칩n le pasamos **nuevos datos** con los que va a tratar de hacer predicciones a partir de lo que aprendi칩 de los datos de entrenamiento. Para esto ingresamos nuestros datos, que ya no est치n necesariamente etiquetados; pasan por el extractor de atributos y a partir de los features que reconoce como m치s importantes trata de realizar predicciones. De forma que al final del proceso obtenemos una etiqueta que nos 칤ndica a que clase pertenece."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Puede que en la mayor칤a de etapas de nuestro modelo tengamos que trabajar con probabilidades para realizar nuestra predicciones aunque no siempre tiene que ser as칤, esto depender치 siempre del dise침o que elijamos para nuestro algoritmo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://i.imgur.com/6Fy93Tm.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez que seleccionamos el **dise침o** de nuestro modelo procedemos a definir la etapa de entrenamiento, es decir, como va a realizar el proceso de la asignaci칩n de los valores de nuestros inputs para ciertos outputs. Usualmente se suele utilizar la estimaci칩n por **m치xima verosimilitud** o **MLE** por sus siglas en ingl칠s; la cual es una t칠cnica que se usa para ajustar nuestro modelo y estimar sus par치metros, de forma que a partir de esa t칠cnica nuestro modelo aprender치 a asignar probabilidades a cada una de nuestras posibles ocurrencias de nuestros datos. \n",
    "\n",
    "Despu칠s pasamos por una **etapa de calibraci칩n**, donde lo que calibramos no son nuestros par치metros en s칤 (de eso ya se encarga el entrenamiento), si no que ahora tratamos de minimizar los errores dados por variables externas al proceso de optimizaci칩n, es decir, errores que vienen dados por **hiper-par치metros** (par치metros fuera de nuestro esquema de optimizaci칩n).\n",
    "\n",
    "Finalmente pasamos por un **proceso de interpretaci칩n** de la predicci칩n realizada, por que a pesar de que al final lo que estamos obteniendo solo son n칰meros o probabilidades, muchas veces viene como resultado de un proceso sumamente complejo y nos puede resultar muy dif칤cil o confuso el interpretar los resultados. Por ejemplo, si estamos recibiendo probabilidades como valores de salida, muchas veces tenemos que reinterpretar esos datos tambi칠n de forma probabil칤stica para llegar a una conclusi칩n clara."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fundamentos de probabilidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Tabla de contenido 游눞",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "279.273px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
